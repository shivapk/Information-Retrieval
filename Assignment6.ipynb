{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 3:  Classification Cook-off: Naive Bayes vs Rocchio (plus a little bit of recommenders)\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Wednesday, March 29 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Hands-on practice building and evaluating classifiers.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as YOUR_UIN_hw3.ipynb. Submit this notebook via eCampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Saturday April 1 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Yelp review data\n",
    "\n",
    "In this assignment, given a Yelp review, your task is to implement two classifiers to predict if the business category of this review is \"food-relevant\" or not, **only based on the review text**. The data is from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "## Build the training data\n",
    "\n",
    "First, you will need to download this data file as your training data: [training_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMdzBVTndwenoxQlk) \n",
    "\n",
    "The training data file includes 40,000 Yelp reviews. Each line is a json-encoded review, and **you should only focus on the \"text\" field**. As same as in homework 1, you should tokenize the review text by using the regular expression \"\\W+\" (we discussed it in [this Piazza post](https://piazza.com/class/ixkk1fy863r1vs?cid=29). Do NOT remove stop words. **Do casefolding but no stemming**.\n",
    "\n",
    "The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**.\n",
    "\n",
    "## Testing data\n",
    "\n",
    "We provide 100 yelp reviews here: [testing_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMbXdyTkhrZDN4Wms). The testing data file has the same format as the training data file. Again, you can get the label informaiton in the \"label\" field. Only use it when you evalute your classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Naive Bayes classifier [35 points]\n",
    "\n",
    "In this part, you will implement a Naive Bayes classifier, which outputs the probabilities that a given review belongs to each class.\n",
    "\n",
    "Use a mixture model that mixes the probability from the document with the general collection frequency of the word. **You should use lambda = 0.7**. Be careful about the decimal rounding since multiplying many probabilities can generate a tiny value. We will not grade on the exact probability value, so feel free to change to logorithm summation (it's not required, though). If the tie case happens, **always go to the \"Food-irrelevant\" side**.\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the naive bayes classifier\n",
    "# Insert as many cells as you want\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "lamda = 0.7\n",
    "klassesCount = defaultdict(lambda: 0.0)\n",
    "totalDocs = 0\n",
    "vocab = set()\n",
    "klassTotalWordCount = defaultdict(lambda: 0.0)\n",
    "klassWordCount = defaultdict(lambda: 0.0)\n",
    "klasses = [\"Food-relevant\", \"Food-irrelevant\"]\n",
    "\n",
    "def add_review(review, klass):\n",
    "    global klassesCount, totalDocs, vocab, klassTotalWordCount, klassWordCount\n",
    "    totalDocs += 1\n",
    "    klassesCount[klass] += 1\n",
    "    #review = review.lower()\n",
    "    temp_list = re.split('\\W+', review)\n",
    "    temp_list = filter(None, temp_list)\n",
    "    for word in temp_list:\n",
    "        word = word.lower()\n",
    "        if word == \"\":\n",
    "            continue\n",
    "        vocab.add(word)\n",
    "        klassTotalWordCount[klass] += 1\n",
    "        klassWordCount[(klass, word)] += 1\n",
    "\n",
    "def parse_json():\n",
    "    with open('training_data.json') as data_file:\n",
    "        for line in data_file:\n",
    "            data = json.loads(line)\n",
    "            klass = data[\"label\"]\n",
    "            review = data[\"text\"]\n",
    "            add_review(review, klass)\n",
    "\n",
    "parse_json()\n",
    "#print \"vocab size : \", len(vocab)\n",
    "#print \"relevant tokens : \", klassTotalWordCount[klasses[0]], \"  irrelevant : \", klassTotalWordCount[klasses[1]]\n",
    "            \n",
    "def nb_classify(review):\n",
    "    klass_prob = {}\n",
    "    review = review.lower()\n",
    "    temp_list = re.split('\\W+', review)\n",
    "    temp_list = filter(None, temp_list)\n",
    "    for klass in klasses:\n",
    "        klass_prob[klass] = math.log10(float(klassesCount[klass]) / totalDocs)\n",
    "        for word in temp_list:\n",
    "            if word == \"\" or word not in vocab:\n",
    "                continue\n",
    "            #word = word.lower()\n",
    "            num2 = num1 = klassWordCount[(klass, word)]\n",
    "            den2 = den1 = klassTotalWordCount[klass]\n",
    "            if klass == klasses[0]:\n",
    "                num2 += klassWordCount[(klasses[1], word)]\n",
    "                den2 += klassTotalWordCount[klasses[1]]\n",
    "            else:\n",
    "                num2 += klassWordCount[(klasses[0], word)]\n",
    "                den2 += klassTotalWordCount[klasses[0]]\n",
    "            klass_prob[klass] += math.log10(lamda*(float(num1) / den1) + (1 - lamda)*(float(num2) / den2))\n",
    "    return klass_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.89\n",
      "Food-relevant  : \n",
      "Precision :  0.901408450704\n",
      "Recall :  0.941176470588\n",
      "Food-irrelevant  : \n",
      "Precision :  0.862068965517\n",
      "Recall :  0.78125\n"
     ]
    }
   ],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "def test():\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    with open('testing_data.json') as data_file:\n",
    "        total_docs = 0.0\n",
    "        correct_docs = 0.0\n",
    "        incorrect_docs = {}\n",
    "        for line in data_file:\n",
    "            total_docs += 1\n",
    "            data = json.loads(line)\n",
    "            klass_prob = nb_classify(data[\"text\"])\n",
    "            predicted_class = None\n",
    "            max_prob = -sys.maxint -1\n",
    "            for klass in klasses:\n",
    "                temp_prob = klass_prob[klass]\n",
    "                if temp_prob >= max_prob:\n",
    "                    max_prob = temp_prob\n",
    "                    predicted_class = klass\n",
    "            #print klass_prob[klasses[0]], \" \", klass_prob[klasses[1]], \" \", predicted_class, \" \", data[\"label\"]\n",
    "            if predicted_class == data[\"label\"]:\n",
    "                correct_docs += 1\n",
    "                confusion_matrix[predicted_class][\"True\"] += 1\n",
    "            else:\n",
    "                incorrect_docs[total_docs] = (data[\"text\"], data[\"label\"])\n",
    "                confusion_matrix[predicted_class][\"False\"] += 1\n",
    "    print \"accuracy : \", correct_docs / total_docs\n",
    "    print klasses[0], \" : \"\n",
    "    print \"Precision : \", confusion_matrix[klasses[0]][\"True\"] / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])\n",
    "    print \"Recall : \", confusion_matrix[klasses[0]][\"True\"] / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "    \n",
    "    print klasses[1], \" : \"\n",
    "    print \"Precision : \", confusion_matrix[klasses[1]][\"True\"] / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "    print \"Recall : \", confusion_matrix[klasses[1]][\"True\"] / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])\n",
    "    '''\n",
    "    for key in incorrect_docs:\n",
    "        #print key, \" \", incorrect_docs[key]\n",
    "        print key\n",
    "    '''\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Rocchio classifier [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, your job is to implement a Rocchio classifier for \"food-relevant vs. food-irrelevant\". You need to aggregate all the reviews of each class, and find the center. **Use the normalized raw term frequency**.\n",
    "\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magnitude(u):\n",
    "    return math.sqrt(sum(u[i]*u[i] for i in range(len(u))))\n",
    "\n",
    "def magnitude_map(u):\n",
    "    return math.sqrt(sum(u[i]*u[i] for i in u))\n",
    "\n",
    "def normalize(u):\n",
    "    mag = magnitude(u)\n",
    "    if mag == 0:\n",
    "        return u\n",
    "    return [float(u[i])/mag for i in range(len(u))]\n",
    "\n",
    "def normalize_map(u):\n",
    "    mag = magnitude_map(u)\n",
    "    if mag == 0:\n",
    "        return u\n",
    "    for i in u:\n",
    "        u[i] = float(u[i])/mag\n",
    "    return u\n",
    "\n",
    "def centroid(u, num_docs):\n",
    "    return [float(u[i])/num_docs for i in range(len(u))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the Rocchio classifier\n",
    "# Insert as many cells as you want\n",
    "klasses = [\"Food-relevant\", \"Food-irrelevant\"]\n",
    "klass_vec = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "doc_count = defaultdict(lambda: 0.0)\n",
    "vocab = set()\n",
    "\n",
    "def tokenize(review):\n",
    "    review = review.lower()\n",
    "    word_list = re.split('\\W+', review)\n",
    "    word_list = filter(None, word_list)\n",
    "    temp_vec = defaultdict(lambda: 0.0)\n",
    "    for word in word_list:\n",
    "        #word = word.lower()\n",
    "        vocab.add(word)\n",
    "        temp_vec[word] += 1\n",
    "    return normalize_map(temp_vec)\n",
    "\n",
    "def rocchio_training():\n",
    "    with open(\"training_data.json\") as data_file:\n",
    "        for line in data_file:\n",
    "            data = json.loads(line)\n",
    "            label = data[\"label\"]\n",
    "            doc_count[label] += 1\n",
    "            review = data[\"text\"]\n",
    "            doc_vec = tokenize(review)\n",
    "            for word in doc_vec:\n",
    "                klass_vec[label][word] += doc_vec[word]\n",
    "\n",
    "def gen_centroid_vecs():\n",
    "    for klass in klasses:\n",
    "        num_docs = doc_count[klass]\n",
    "        for word in vocab:\n",
    "            klass_vec[klass][word] = float(klass_vec[klass][word]) / num_docs\n",
    "            \n",
    "def manhattan_dist(klass, doc_vec):\n",
    "    dist = 0.0\n",
    "    for word in vocab:\n",
    "        dist += abs(klass_vec[klass][word] - doc_vec[word])\n",
    "    return dist\n",
    "\n",
    "def euclidean_dist(klass, doc_vec):\n",
    "    dist = 0.0\n",
    "    for word in vocab:\n",
    "        dist += ((klass_vec[klass][word] - doc_vec[word])*(klass_vec[klass][word] - doc_vec[word]))\n",
    "    \n",
    "    return math.sqrt(dist)\n",
    "\n",
    "def rocchio_classify(review):\n",
    "    doc_vec = tokenize(review)\n",
    "    min_dist = sys.maxint\n",
    "    predicted_klass = None\n",
    "    for klass in klasses:\n",
    "        '''\n",
    "        dist = 0.0\n",
    "        for word in vocab:\n",
    "            dist += abs(klass_vec[klass][word] - doc_vec[word])\n",
    "        '''\n",
    "        #dist = manhattan_dist(klass, doc_vec)\n",
    "        dist = euclidean_dist(klass, doc_vec)\n",
    "        #print dist\n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            predicted_klass = klass\n",
    "    return predicted_klass\n",
    "\n",
    "\n",
    "rocchio_training()\n",
    "gen_centroid_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "def test():\n",
    "    incorrect_docs = {}\n",
    "    total_docs = 0.0\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    with open(\"testing_data.json\") as data_file:\n",
    "        for line in data_file:\n",
    "            total_docs += 1.0\n",
    "            data = json.loads(line)\n",
    "            review = data[\"text\"]\n",
    "            predicted_klass = rocchio_classify(review)\n",
    "            if predicted_klass != data[\"label\"]:\n",
    "                incorrect_docs[total_docs] = (data[\"text\"], data[\"label\"])\n",
    "                confusion_matrix[predicted_klass][\"False\"] += 1\n",
    "            else:\n",
    "                confusion_matrix[predicted_klass][\"True\"] += 1\n",
    "    \n",
    "    print \"accuracy : \", float(total_docs - len(incorrect_docs)) / total_docs\n",
    "    print klasses[0], \" : \"\n",
    "    print \"Precision : \", float(confusion_matrix[klasses[0]][\"True\"]) / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])\n",
    "    print \"Recall : \", float(confusion_matrix[klasses[0]][\"True\"]) / (confusion_matrix[klasses[0]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "                                                                                       \n",
    "    print klasses[1], \" : \"\n",
    "    print \"Precision : \", float(confusion_matrix[klasses[1]][\"True\"]) / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[1]][\"False\"])\n",
    "    print \"Recall : \", float(confusion_matrix[klasses[1]][\"True\"]) / (confusion_matrix[klasses[1]][\"True\"] + confusion_matrix[klasses[0]][\"False\"])  \n",
    "    '''\n",
    "    #print len(incorrect_docs), \" \", total_docs\n",
    "    for key in incorrect_docs:\n",
    "        #print key, \" : \", incorrect_docs[key]\n",
    "        print key\n",
    "    '''\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Naive Bayes vs. Rocchio [20 points]\n",
    "\n",
    "Which method gives the better results? In terms of what? How did you compare them? Can you explain why you observe what you do? Write 1-3 paragraphs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "\n",
    "Here, Naive Bayes gives out better results than Rocchio (for all accuracy, precision and recall) as shown by results below. \n",
    "\n",
    "NAIVE BAYES  - \n",
    "\n",
    "accuracy :  0.89\n",
    "\n",
    "Food-relevant  : \n",
    "\n",
    "Precision :  0.901408450704\n",
    "\n",
    "Recall :  0.941176470588\n",
    "\n",
    "Food-irrelevant  : \n",
    "\n",
    "Precision :  0.862068965517\n",
    "\n",
    "Recall :  0.78125\n",
    "\n",
    "ROCCHIO - \n",
    "\n",
    "accuracy :  0.67\n",
    "\n",
    "Food-relevant  : \n",
    "\n",
    "Precision :  0.818181818182\n",
    "\n",
    "Recall :  0.661764705882\n",
    "\n",
    "Food-irrelevant  : \n",
    "\n",
    "Precision :  0.488888888889\n",
    "\n",
    "Recall :  0.6875\n",
    "\n",
    "Naive bayes is a language model(generative model), whereas Rocchio is a classification algorithm. Rochio can sometimes lead to bad(confused) results. This is because, in Rocchio, we calculate the centroids of the documents in each class. If the distribution of the documents in a class is not consistent, like the docs are distributed in two clusters far away, then the centroid will lie in between. Thus, it may happen that the centroid of one class may be lying close to documents of other class. And thus, classification will be wrong here. \n",
    "\n",
    "Naive bayes is a probabilistic model which is immune from this problem. Thus naive bayes is quite robust as compared to Rocchio. Rocchio is preferred when we know the distribution of documents belonging to different classes in advance. Naive Bayes is not affected by the distribution of the data. Also, Rocchio is not preferred in case of multimodal distribution\n",
    "\n",
    "Also, I observed that the Rocchio results are getting affected by the way I calculate the distance(manhattan/eucledian) or the way I normalize the vector. There is no such variation in Naive Bayes.\n",
    "For Rocchio, accuracy also depends on the fact that how we normalize and represent the doc vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Recommenders [10 points]\n",
    "\n",
    "Finally, since we've begun our discussion of recommenders, let's do a quick problem too:\n",
    "\n",
    "The table below is a utility matrix, representing the ratings, on a 1â€“5 star scale, of eight items, *a* through *h*, by three users *A*, *B*, and *C*. \n",
    "<pre>\n",
    "\n",
    "  | a  b  c  d  e  f  g  h\n",
    "--|-----------------------\n",
    "A | 4  5     5  1     3  2\n",
    "B |    3  4  3  1  2  1\n",
    "C | 2     1  3     4  5  3\n",
    "\n",
    "</pre>\n",
    "\n",
    "Compute the following from the data of this matrix.\n",
    "\n",
    "(a) Treating the utility matrix as boolean, compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(b) Repeat Part (a), but use the cosine distance.\n",
    "\n",
    "(c) Treat ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0. Compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(d) Repeat Part (c), but use the cosine distance.\n",
    "\n",
    "(e) Normalize the matrix by subtracting from each nonblank entry the average\n",
    "value for its user.\n",
    "\n",
    "(f) Using the normalized matrix from Part (e), compute the cosine distance\n",
    "between each pair of users.\n",
    "\n",
    "(g) Which of the approaches above seems most reasonable to you? Give a one or two sentence argument supporting your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "\n",
    "(a)A-B = 1/2, B-C = 1/2, A-C = 1/2\n",
    "\n",
    "(b)A-B = 2/3, B-C = 2/3, A-C = 2/3\n",
    "\n",
    "(c)A-B = 2/5, B-C = 1/6, A-C = 2/6\n",
    "\n",
    "(d)A-B = 1/1.73 or 1/sqrt(3), B-C = 1/3.46 or 1/(2*sqrt(3)), A-C = 1/2\n",
    "\n",
    "(e)\n",
    "A - avg = 3.33\n",
    "B - avg = 2.33\n",
    "C - avg = 3.0\n",
    "<pre>\n",
    "\n",
    "  |  a    b    c    d     e     f     g    h\n",
    "--|--------------------------------------------\n",
    "A | 0.67 1.67      1.67 -2.33       -0.33 -1.33\n",
    "B |      0.67 1.67 0.67 -1.33 -0.33 -1.33\n",
    "C |-1.0      -2.0  0.0         1.0   2.0   0.0\n",
    "\n",
    "</pre>\n",
    "\n",
    "(f)cos(A-B) = (1.67*0.67 + 1.67*0.67 + -2.33*-1.33 + -0.33*-1.33) / (sqrt(0.67*0.67 + 1.67*1.67 + 1.67*1.67 + -2.33*-2.33 + -0.33*-0.33 + -1.33*-1.33) * sqrt(0.67*0.67 + 1.67*1.67 + 0.67*0.67 + -1.33*-1.33 + -0.33*-0.33 + -1.33*-1.33)) = 0.58408219372\n",
    "\n",
    "cos(B-C) = (1.67*-2.0 + -0.33*1.0 + -1.33*2.0) / (sqrt(0.67*0.67 + 1.67*1.67 + 0.67*0.67 + -1.33*-1.33 + -0.33*-0.33 + -1.33*-1.33) * sqrt(-1.0*-1.0 + -2.0*-2.0 + 1.0*1.0 + 2.0*2.0)) = -0.73918138757\n",
    "\n",
    "cos(A-C) = (0.67*-1.0 + -0.33*2.0) / (sqrt(0.67*0.67 + 1.67*1.67 + 1.67*1.67 + -2.33*-2.33 + -0.33*-0.33 + -1.33*-1.33) * sqrt(-1.0*-1.0 + -2.0*-2.0 + 1.0*1.0 + 2.0*2.0)) = -0.11518109075\n",
    "\n",
    "(g) Cosine distances calculated from the normalized matrix make most sense to me(last approach). This is followed by approach in part d and approach in part c. \n",
    "    - Here, as seen from the results, A-B are most similar followed by A-C and B-C. This is also evident if we eyeball the matrix.\n",
    "    - Normalizing the matrix introduces consistency, sanity in the ratings. This can be seen as if a user ,on an average, rates movies highly while other user berates movies highly. This brings them to the same level.\n",
    "    - cosine similarity takes absolute values in account instead of the boolean values like in Jaccard. So, in Jaccard, we lose some of the critical info which gets preserved and accounted for in Cosine similarity.\n",
    "    - In the third approach, we considered the normalized values, which resulted into most reasonable similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
