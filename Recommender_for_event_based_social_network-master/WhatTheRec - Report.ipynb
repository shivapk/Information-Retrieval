{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Report\n",
    "# Project Name - WhatTheRec\n",
    "\n",
    "## Team Members: \n",
    "### Shiva Kumar Pentyala, Sai Kiran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Problem Statement\n",
    "In the present scenario, a plethora of events is being held daily at different places and different times. Hence, it is difficult for a user to keep track of all the events and choose between various events to attend. The problem increases because the users do not know which events would be interesting and relevant to them. Hence, it is important that an event sharing social network uses recommenders to suggest relevant and interesting events to its users which help them choose between various events. While recommending events, another problem that arises is that of unseen data(events). We cannot say, confidently, which users will attend what events, especially in the case of new users. This is called the cold start problem. There are many Event Based Social Network (EBSN) websites like meetup.com, eventbrite.com etc., which could deploy this algorithm in order to successfully suggest interesting and relevant events to its users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "There has been a lot of work already done including hybrid approaches in event recommendation by Minkov et al. [1]. This paper demonstrates collaborative filtering on a dataset of academic events. Our implementation of the recommender system follows [2], which also utilizes other signals such as a group of users, location, and temporal preferences. Khrouf and Troncy [3] for recommending music related events, utilizes category information about different artists from a well-known source. But this approach fails when we do not have this information about every event, as events can be across different domains. Our implementation takes into account RSVPs, group information and other contextual data. Recent works [4] have shown that pure matrix factorization (based only on user-event interactions) performs poorly on EBSN data in comparison to other methods due to high level of the sparsity of these datasets. As per experiments carried out, state-of-the-art matrix factorization algorithms did not perform better than simple collaborative filtering algorithms such as user-based k-NN. Thus, our focus is on considering the explicit features than the latent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach & Methods\n",
    "\n",
    "Data Acquisition - Initially we fetched the data from meetup.com using their public APIs. As we started developing, we realized that the data was quite less as it is super sparse in nature. And, when we again started getting data, they changed their APIs and a lot of them were broken. So, we decided to contact the authors of this paper and got the raw data from them. We cleaned and parsed the data and used that for our project. We used data from Jan 1, 2010 - Jan 2014. \n",
    "The data contains following information:\n",
    "\n",
    "1. Events: event id, description of the event, event group, users who RSVP'd this event.\n",
    "2. Members: member id, latitude, longitude\n",
    "3. Groups: events organized by the group, members involved in the group\n",
    "\n",
    "We, then, divided the data into timestamps of 6 months. For example, one timestamp was July 1, 2010. The first 6 months were used for training and the last 6 months for testing. We extracted the following features to recommend events: time, location, description and group frequency of the events. \n",
    "\n",
    "a. Content-based - We formed a model of a user using the description of the past events he/she had attended. Then, we found the similarity of potential events with the user model. \n",
    "\n",
    "b. Location-based - We fit a gaussian distribution to the location of the past events of a user. Then, we found the probability of new event according to the curve. \n",
    "\n",
    "c. Group-Frequency - The intuition here is that the likelihood of the target user attending an event depends on the number of events this user attended in the group that the event belongs to. \n",
    "\n",
    "d. Time-Aware - Another important factor that affects users' decision on attending an event is when the event occurs. We capture this intuition by assuming that users that attended events in the past on certain days of the week and at certain hours of the day will likely attend events with a similar temporal profile in the future.\n",
    "\n",
    "Now, that we have built a model of the data, we then computed similarity scores for each member with the events that they had rsvp'd. Events in the first half were used to train the model and the events in the second half were used to test it. Note that we knew the events which the members had rsvp'd for the both the halves. So, the rsvp's for the second half were used to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run our recommender code below. Type the name of city you want to run the recommender on. \n",
    "\n",
    "Note: We have data only from Chicago, San Jose and Phoenix. You can also change the number of members you want to train on. Additionally, you can use a list of ML algorithms as well.\n",
    "\n",
    "<b>Note: Please do not run this code outside the current directory. The code in this notebook calls some python code that are present in the subdirectories.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Parameters Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('src')\n",
    "city = 'LCHICAGO' # or 'LSAN JOSE', or 'LPHOENIX'\n",
    "algolist = ['rf'] #'svm', 'nb', 'mlp'\n",
    "number_of_members = 50 #100, 150 (not more than this, otherwise system would take too much memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "import argparse\n",
    "from partition import *\n",
    "from content.content_recommender import ContentRecommender\n",
    "from temporal.time_recommender import TimeRecommender\n",
    "from location.location_recommender import LocationRecommender\n",
    "from group_frequency.grp_freq_recommender import GrpFreqRecommender\n",
    "from hybrid.learning_to_rank import LearningToRank\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "#number of seconds in 6 months\n",
    "train_data_interval = ((364 / 2) * 24 * 60 * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing using Content Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def content_classifier(training_repo, test_repo, timestamp, simscores, test_members):\n",
    "\n",
    "    training_events_dict = training_repo['members_events']\n",
    "    potential_events = list(test_repo['events_info'].keys())\n",
    "\n",
    "    contentRecommender = ContentRecommender()\n",
    "    contentRecommender.train(training_events_dict, training_repo)\n",
    "    test_events_vec = contentRecommender.get_test_events_wth_description(test_repo, potential_events)\n",
    "\n",
    "    #TEST FOR BEST USERS\n",
    "    for member in test_members:\n",
    "         contentRecommender.test(member, potential_events, test_events_vec, simscores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing using Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_classifier(training_repo, test_repo, timestamp, simscores, test_members):\n",
    "\n",
    "    training_events_dict = training_repo['members_events']\n",
    "    potential_events = list(test_repo['events_info'].keys())\n",
    "\n",
    "    timeRecommender = TimeRecommender()\n",
    "    timeRecommender.train(training_events_dict, training_repo)\n",
    "    test_events_vec = timeRecommender.get_test_event_vecs_with_time(test_repo, potential_events)\n",
    "\n",
    "    #TEST FOR BEST USERS\n",
    "    for member in test_members:\n",
    "         timeRecommender.test(member, potential_events, test_events_vec, simscores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing using Location feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loc_classifier(training_repo, test_repo, timestamp, simscores, test_members):\n",
    "    training_events_dict = training_repo['members_events']\n",
    "    potential_events = list(test_repo['events_info'].keys())\n",
    "\n",
    "    locationRecommender = LocationRecommender()\n",
    "    locationRecommender.train(training_events_dict, training_repo)\n",
    "\n",
    "    #TEST FOR BEST USERS\n",
    "    for member in test_members:\n",
    "        locationRecommender.test(member, potential_events, test_repo, simscores)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing using Group Frequency feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def grp_freq_classifier(training_repo, test_repo, timestamp, simscores, test_members):\n",
    "    training_events_dict = training_repo['members_events']\n",
    "    potential_events = list(test_repo['events_info'].keys())\n",
    "\n",
    "    grp_freq_recommender = GrpFreqRecommender()\n",
    "    grp_freq_recommender.train(training_events_dict, training_repo)\n",
    "    \n",
    "    for member in test_members:\n",
    "         grp_freq_recommender.test(member, potential_events, test_repo, simscores)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extract the data from csv file into python dictionaries. local_crawler.py performs this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_and_run_local_crawler():\n",
    "    \n",
    "    if os.path.isdir(\"../crawler/cities/LCHICAGO\") and os.path.isdir(\"../crawler/cities/LSAN JOSE\")\\\n",
    "            and os.path.isdir(\"../crawler/cities/LPHOENIX\"):\n",
    "        if len(os.listdir(\"../crawler/cities/LCHICAGO\")) >= 5 and len(os.listdir(\"../crawler/cities/LSAN JOSE\"))>=5\\\n",
    "                and len(os.listdir(\"../crawler/cities/LPHOENIX\")) >= 5:\n",
    "            return\n",
    "        \n",
    "    # Run the local_crawler.py file    \n",
    "    os.chdir(\"../crawler\")\n",
    "    os.system(\"python local_crawler.py\")\n",
    "    os.chdir(\"../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a list of best users based on the number of RSVP's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_script(number_of_members):\n",
    "    os.chdir(\"scripts\")\n",
    "    os.system(\"python script.py --number \" + str(number_of_members))\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building best user database ...\n",
      "Best users extracted.\n",
      "Partition at timestamp  2013-06-27 19:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2013-06-27 19:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  183.34123  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  6.515163  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  203.89219  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  32.044853  seconds\n",
      "============== Starting classification for partition : 1 ===================\n",
      "RF -> Precision : 0.695475638051 Recall : 0.872634643377 F-measure : 0.774047772757\n",
      "============== Completed classification for partition : 1 ===================\n",
      "Partition at timestamp  2012-12-27 18:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2012-12-27 18:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  124.92545  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  4.271634  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  139.301343  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  3.655516  seconds\n",
      "============== Starting classification for partition : 2 ===================\n",
      "RF -> Precision : 0.660746003552 Recall : 0.749244712991 F-measure : 0.702218027371\n",
      "============== Completed classification for partition : 2 ===================\n",
      "Partition at timestamp  2012-06-28 19:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2012-06-28 19:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  102.239596  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  3.740986  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  115.317304  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  2.808245  seconds\n",
      "============== Starting classification for partition : 3 ===================\n",
      "RF -> Precision : 0.738186462324 Recall : 0.544256120527 F-measure : 0.626558265583\n",
      "============== Completed classification for partition : 3 ===================\n",
      "Partition at timestamp  2011-12-29 18:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2011-12-29 18:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  75.745748  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  2.862532  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  98.76985  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  2.607118  seconds\n",
      "============== Starting classification for partition : 4 ===================\n",
      "RF -> Precision : 0.507448789572 Recall : 0.581023454158 F-measure : 0.541749502982\n",
      "============== Completed classification for partition : 4 ===================\n",
      "Partition at timestamp  2011-06-30 19:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2011-06-30 19:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  72.479696  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  2.50425  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  81.976913  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  2.38859  seconds\n",
      "============== Starting classification for partition : 5 ===================\n",
      "RF -> Precision : 0.673548387097 Recall : 0.509765625 F-measure : 0.580322401334\n",
      "============== Completed classification for partition : 5 ===================\n",
      "Partition at timestamp  2010-12-30 18:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2010-12-30 18:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  48.489081  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  2.047906  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  69.998562  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  1.956901  seconds\n",
      "============== Starting classification for partition : 6 ===================\n",
      "RF -> Precision : 0.712121212121 Recall : 0.488196411709 F-measure : 0.579271708683\n",
      "============== Completed classification for partition : 6 ===================\n",
      "Partition at timestamp  2010-07-01 19:00:00  are : \n",
      "Partitioned Repo retrieved for timestamp :  2010-07-01 19:00:00\n",
      "Starting Content Classifier\n",
      "Completed Content Classifier in  45.598219  seconds\n",
      "Starting Time Classifier\n",
      "Completed Time Classifier in  1.815603  seconds\n",
      "Starting Location Classifier\n",
      "Completed Location Classifier in  60.277625  seconds\n",
      "Starting Group Frequency Classifier\n",
      "Completed Group Frequency Classifier in  1.59685  seconds\n",
      "============== Starting classification for partition : 7 ===================\n",
      "RF -> Precision : 0.485947416138 Recall : 0.619653179191 F-measure : 0.544715447154\n",
      "============== Completed classification for partition : 7 ===================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    check_and_run_local_crawler()\n",
    "    print \"Building best user database ...\"\n",
    "    run_script(number_of_members)\n",
    "    print \"Best users extracted.\"\n",
    "\n",
    "    \n",
    "    group_members, group_events, event_group = load_groups(\"../crawler/cities/\" + city + \"/group_members.json\",\n",
    "                                                            \"../crawler/cities/\" + city + \"/group_events.json\")\n",
    "    events_info = load_events(\"../crawler/cities/\" + city + \"/events_info.json\")\n",
    "    members_info = load_members(\"../crawler/cities/\" + city + \"/members_info.json\")\n",
    "    member_events = load_rsvps(\"../crawler/cities/\" + city + \"/rsvp_events.json\")\n",
    "\n",
    "    repo = dict()\n",
    "    repo['group_events'] = group_events\n",
    "    repo['group_members'] = group_members\n",
    "    repo['events_info'] = events_info\n",
    "    repo['members_info'] = members_info\n",
    "    repo['members_events'] = member_events\n",
    "    repo['event_group'] = event_group\n",
    "    \n",
    "    #simscores_across_features is a dictionary to store similarity score obtained for each feature\n",
    "    #for each member and for a given event. For example in case of content classifer we will\n",
    "    #access the similarity score as follows: simscores['content_classifier'][member_id][event_id].\n",
    "    #We will pass only a specific subdictionary (Ex: simscores['content_classifier']) to the\n",
    "    #classifier functions, which will work on them and populate them.\n",
    "    \n",
    "    simscores_across_features = defaultdict(lambda :defaultdict(lambda :defaultdict(lambda :0)))\n",
    "    hybrid_simscores = defaultdict(lambda :defaultdict(lambda :0))\n",
    "\n",
    "    start_time = 1262304000 # 1st Jan 2010\n",
    "    end_time = 1388534400 # 1st Jan 2014\n",
    "    timestamps = get_timestamps(start_time, end_time)\n",
    "    timestamps = sorted(timestamps, reverse=True)\n",
    "    count_partition = 1\n",
    "\n",
    "    f_temp = open('temp_result.txt', 'w+')\n",
    "    f_temp.write(\"Using classification algorithms : \" + str(algolist) + \" and number of members as : \" +\\\n",
    "                 str(number_of_members) + \"\\n\")\n",
    "\n",
    "    for t in timestamps:\n",
    "        start_time = t - train_data_interval\n",
    "        end_time = t + train_data_interval\n",
    "        test_members = []\n",
    "        f = open(\"scripts/\"+city + \"_best_users_\" + str(start_time) + \"_\" + str(end_time) + \".txt\", \"r\")\n",
    "        for users in f:\n",
    "            test_members.extend(users.split())\n",
    "        f.close()\n",
    "        test_members = test_members[:number_of_members]\n",
    "        print \"Partition at timestamp \", datetime.datetime.fromtimestamp(t), \" are : \"\n",
    "        training_repo, test_repo = get_partitioned_repo_wrapper(t, repo)\n",
    "        print \"Partitioned Repo retrieved for timestamp : \", datetime.datetime.fromtimestamp(t)\n",
    "\n",
    "        training_members = set(training_repo['members_events'].keys())\n",
    "        test_members =  training_members.intersection(set(test_members))\n",
    "        test_members = list(test_members)\n",
    "        \n",
    "        #Calling content based classifer train and test functions from here. Pass the repo\n",
    "        #as an argument to these functions.\n",
    "        start = time.clock()\n",
    "        print \"Starting Content Classifier\"\n",
    "        content_classifier(training_repo, test_repo, t, simscores_across_features['content_classifier'],\\\n",
    "                           test_members)\n",
    "        print \"Completed Content Classifier in \", time.clock() - start, \" seconds\"\n",
    "\n",
    "        start = time.clock()\n",
    "        print \"Starting Time Classifier\"\n",
    "        time_classifier(training_repo, test_repo, t, simscores_across_features['time_classifier'], test_members)\n",
    "        print \"Completed Time Classifier in \", time.clock() - start, \" seconds\"\n",
    "\n",
    "        start = time.clock()\n",
    "        print \"Starting Location Classifier\"\n",
    "        loc_classifier(training_repo, test_repo, t, simscores_across_features['location_classifier'], test_members)\n",
    "        print \"Completed Location Classifier in \", time.clock() - start, \" seconds\"\n",
    "\n",
    "        start = time.clock()\n",
    "        print \"Starting Group Frequency Classifier\"\n",
    "        grp_freq_classifier(training_repo, test_repo, t, simscores_across_features['grp_freq_classifier'],\\\n",
    "                            test_members)\n",
    "        print \"Completed Group Frequency Classifier in \", time.clock() - start, \" seconds\"\n",
    "\n",
    "        f_temp.write(\"============== Starting classification for partition : \" +  str(count_partition) + \\\n",
    "                     \" ===================\\n\")\n",
    "        print \"============== Starting classification for partition : \" +  str(count_partition) + \\\n",
    "        \" ===================\"\n",
    "        learningToRank = LearningToRank()\n",
    "        learningToRank.learning(simscores_across_features, test_repo[\"events_info\"].keys(), \\\n",
    "                                test_repo[\"members_events\"], test_members, f_temp, algolist, \\\n",
    "                                number_of_members, count_partition)\n",
    "        f_temp.write(\"============== Completed classification for partition : \" +  str(count_partition) + \\\n",
    "                     \" ===================\\n\")\n",
    "        print \"============== Completed classification for partition : \" + str(count_partition) + \\\n",
    "        \" ===================\"\n",
    "        \n",
    "        count_partition += 1\n",
    "    \n",
    "    f_temp.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We divided an year's data into 2 slots. We built the model of the user from the rsvp'd events in the first slot, and then for each of the event in the next slot, we performed the recommendation. Then, we did a k fold setup to evaluate recommendations. For 80% of the users, we trained the classifier and then calculated precision, recall and F-score for the remaining 20% users.\n",
    "\n",
    "For a new user (who didn't attend any event in the fist slot), we recommend the event which occurs closest to the user's location.\n",
    "\n",
    "We chose three cities in the USA for performing the evaluation, namely, Chicago, Phoenix and San Jose. There are 2 main reasons why we selected these 3 cities:- \n",
    "\n",
    "(1) They are among the most populous cities in the USA, which indicate large event activity \n",
    "\n",
    "(2) The cities are located in different states, thus representing some degree of cultural diversity. \n",
    "\n",
    "Following are the results for these cities. The results were captured on the basis of partition number and number of members on which evaluation was done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results for Chicago:-</h3>\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><th>Partition Number</th><th>Number of members</th><th>Precision</th><th>Recall</th><th>F-measure</th></tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">1</td>\n",
    "<td>50</td>\n",
    "<td>0.695475638051</td>\n",
    "<td>0.872634643377</td>\n",
    "<td>0.774047772757</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.700596056855</td>\n",
    "<td>0.648556876061</td>\n",
    "<td>0.673572845493</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.537199542159</td>\n",
    "<td>0.529721595184</td>\n",
    "<td>0.533434362569</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    " 2</td>\n",
    "<td>50</td>\n",
    "<td>0.660746003552</td>\n",
    "<td>0.749244712991</td>\n",
    "<td>0.702218027371</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.498137307078</td>\n",
    "<td>0.484221417486</td>\n",
    "<td>0.491080797482</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.538929440389</td>\n",
    "<td>0.427812650893</td>\n",
    "<td>0.476985195155</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">3</td>\n",
    "<td>50</td>\n",
    "<td>0.738186462324</td>\n",
    "<td>0.544256120527</td>\n",
    "<td>0.626558265583</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.667063020214</td>\n",
    "<td>0.270752895753</td>\n",
    "<td>0.385169927909</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.458050847458</td>\n",
    "<td>0.426598263615</td>\n",
    "<td>0.441765427054</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">4</td>\n",
    "<td>50</td>\n",
    "<td>0.507448789572</td>\n",
    "<td>0.581023454158</td>\n",
    "<td>0.541749502982</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.660091047041</td>\n",
    "<td>0.26395631068</td>\n",
    "<td>0.37711313394</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.711198428291</td>\n",
    "<td>0.2145820984</td>\n",
    "<td>0.329690346084</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">5</td>\n",
    "<td>50</td>\n",
    "<td>0.673548387097</td>\n",
    "<td>0.509765625</td>\n",
    "<td>0.580322401334</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.759791122715</td>\n",
    "<td>0.178199632578</td>\n",
    "<td>0.28869047619</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.518197573657</td>\n",
    "<td>0.19606557377</td>\n",
    "<td>0.28449096099</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">6</td>\n",
    "<td>50</td>\n",
    "<td>0.712121212121</td>\n",
    "<td>0.488196411709</td>\n",
    "<td>0.579271708683</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.571045576408</td>\n",
    "<td>0.167189952904</td>\n",
    "<td>0.258652094718</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.545180722892</td>\n",
    "<td>0.209612044007</td>\n",
    "<td>0.302802174822</td>\n",
    "</tr>\n",
    " <tr>\n",
    "<td rowspan=\"3\">7</td>\n",
    "<td>50</td>\n",
    "<td>0.485947416138</td>\n",
    "<td>0.619653179191</td>\n",
    "<td>0.544715447154</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.563492063492</td>\n",
    "<td>0.214016578749</td>\n",
    "<td>0.310212998362</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.551656920078</td>\n",
    "<td>0.16915720263</td>\n",
    "<td>0.258920402562</td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results for Pheonix:-</h3>\n",
    "<table>\n",
    "<tr>\n",
    "<th>\n",
    "Partition Number\n",
    "</th>\n",
    "<th>\n",
    "Number of members\n",
    "</th>\n",
    "<th>\n",
    "Precision\n",
    "</th>\n",
    "<th>\n",
    "Recall\n",
    "</th>\n",
    "<th>\n",
    "F-measure\n",
    "</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "1\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.48322147651\n",
    "</td>\n",
    "<td>\n",
    "0.356435643564\n",
    "</td>\n",
    "<td>\n",
    "0.410256410256\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.462082912032\n",
    "</td>\n",
    "<td>\n",
    "0.536594911937\n",
    "</td>\n",
    "<td>\n",
    "0.496559217675\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.417532640554\n",
    "</td>\n",
    "<td>\n",
    "0.532630863358\n",
    "</td>\n",
    "<td>\n",
    "0.468110530246\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "2\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.76376146789\n",
    "</td>\n",
    "<td>\n",
    "0.256351039261\n",
    "</td>\n",
    "<td>\n",
    "0.38386167147\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.499249249249\n",
    "</td>\n",
    "<td>\n",
    "0.348167539267\n",
    "</td>\n",
    "<td>\n",
    "0.410240592227\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.446632782719\n",
    "</td>\n",
    "<td>\n",
    "0.307793345009\n",
    "</td>\n",
    "<td>\n",
    "0.3644375324\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "3\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.512295081967\n",
    "</td>\n",
    "<td>\n",
    "0.59694364852\n",
    "</td>\n",
    "<td>\n",
    "0.551389501544\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.488441461596\n",
    "</td>\n",
    "<td>\n",
    "0.467189728959\n",
    "</td>\n",
    "<td>\n",
    "0.477579292745\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.507783145464\n",
    "</td>\n",
    "<td>\n",
    "0.466929911155\n",
    "</td>\n",
    "<td>\n",
    "0.486500385703\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "4\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.567890691716\n",
    "</td>\n",
    "<td>\n",
    "0.546425636812\n",
    "</td>\n",
    "<td>\n",
    "0.556951423786\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.471113758189\n",
    "</td>\n",
    "<td>\n",
    "0.468047337278\n",
    "</td>\n",
    "<td>\n",
    "0.469575541704\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.472628499791\n",
    "</td>\n",
    "<td>\n",
    "0.50000000000\n",
    "</td>\n",
    "<td>\n",
    "0.485929108485\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "5\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.614572333685\n",
    "</td>\n",
    "<td>\n",
    "0.501291989664\n",
    "</td>\n",
    "<td>\n",
    "0.552182163188\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.446226415094\n",
    "</td>\n",
    "<td>\n",
    "0.521787093216\n",
    "</td>\n",
    "<td>\n",
    "0.481057716756\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.496774193548\n",
    "</td>\n",
    "<td>\n",
    "0.465364946536\n",
    "</td>\n",
    "<td>\n",
    "0.480556889102\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "6\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.589261744966\n",
    "</td>\n",
    "<td>\n",
    "0.612273361227\n",
    "</td>\n",
    "<td>\n",
    "0.600547195622\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.549869904597\n",
    "</td>\n",
    "<td>\n",
    "0.459087617668\n",
    "</td>\n",
    "<td>\n",
    "0.500394632991\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.489247311828\n",
    "</td>\n",
    "<td>\n",
    "0.362756264237\n",
    "</td>\n",
    "<td>\n",
    "0.416612164814\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "7\n",
    "</td>\n",
    "<td>\n",
    "50\n",
    "</td>\n",
    "<td>\n",
    "0.653846153846\n",
    "</td>\n",
    "<td>\n",
    "0.429420505201\n",
    "</td>\n",
    "<td>\n",
    "0.518385650224\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "100\n",
    "</td>\n",
    "<td>\n",
    "0.342857142857\n",
    "</td>\n",
    "<td>\n",
    "0.506181818182\n",
    "</td>\n",
    "<td>\n",
    "0.408810572687\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "150\n",
    "</td>\n",
    "<td>\n",
    "0.399623588457\n",
    "</td>\n",
    "<td>\n",
    "0.367358708189\n",
    "</td>\n",
    "<td>\n",
    "0.3828125\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results for San Jose:-</h3>\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><th>Partition Number</th><th>Number of members</th><th>Precision</th><th>Recall</th><th>F-measure</th></tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">1</td>\n",
    "<td>50</td>\n",
    "<td>0.616191904048</td>\n",
    "<td>0.568858131488</td>\n",
    "<td>0.59157970493</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.517844136926</td>\n",
    "<td>0.458413926499</td>\n",
    "<td>0.486320109439</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.461823573017</td>\n",
    "<td>0.332443970117</td>\n",
    "<td>0.386596338815</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">\n",
    "  2</td>\n",
    "<td>50</td>\n",
    "<td>0.605660377358</td>\n",
    "<td>0.485138539043</td>\n",
    "<td>0.538741258741</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.62962962963</td>\n",
    "<td>0.296511627907</td>\n",
    "<td>0.403162055336</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.598518518519</td>\n",
    "<td>0.333884297521</td>\n",
    "<td>0.428647214854</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">3</td>\n",
    "<td>50</td>\n",
    "<td>0.768229166667</td>\n",
    "<td>0.431602048281</td>\n",
    "<td>0.552693208431</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.705242334322</td>\n",
    "<td>0.427458033573</td>\n",
    "<td>0.532288167227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.763870967742</td>\n",
    "<td>0.304370179949</td>\n",
    "<td>0.435294117647</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">4</td>\n",
    "<td>50</td>\n",
    "<td>0.743690851735</td>\n",
    "<td>0.711698113208</td>\n",
    "<td>0.727342846124</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.777908343126</td>\n",
    "<td>0.433529796988</td>\n",
    "<td>0.55677039529</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.572244897959</td>\n",
    "<td>0.379534380076</td>\n",
    "<td>0.456380208333</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">5</td>\n",
    "<td>50</td>\n",
    "<td>0.781136638452</td>\n",
    "<td>0.622350674374</td>\n",
    "<td>0.692761394102</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.755555555556</td>\n",
    "<td>0.473607038123</td>\n",
    "<td>0.582244254169</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.692610837438</td>\n",
    "<td>0.407536231884</td>\n",
    "<td>0.513138686131</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"3\">6</td>\n",
    "<td>50</td>\n",
    "<td>0.580968280467</td>\n",
    "<td>0.395005675369</td>\n",
    "<td>0.47027027027</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.570502431118</td>\n",
    "<td>0.338787295476</td>\n",
    "<td>0.425120772947</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.555425904317</td>\n",
    "<td>0.318821165439</td>\n",
    "<td>0.405106382979</td>\n",
    "</tr>\n",
    "  <tr>\n",
    "<td rowspan=\"3\">7</td>\n",
    "<td>50</td>\n",
    "<td>0.591549295775</td>\n",
    "<td>0.288858321871</td>\n",
    "<td>0.388170055453</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100</td>\n",
    "<td>0.565723793677</td>\n",
    "<td>0.357894736842</td>\n",
    "<td>0.438426821406</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>150</td>\n",
    "<td>0.551656920078</td>\n",
    "<td>0.16915720263</td>\n",
    "<td>0.258920402562</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observed the importance of different feature vectors when used in Random Forest classifier. Below is the bar graph showing importance of each feature.\n",
    "\n",
    "<img src=\"feature_importance.jpg\">\n",
    "\n",
    "As we can see that the content feature has the most weight. This means that content feature is the most important feature while performing the classification. Also, location of the event and user has a good significance. However, time and group frequency were found to be less important in classification. One of the main reasons why content feature has the most significance is because description of the event is highly rich and it shows the interest of the user. Location is also making a good impact because it recommends the user events based on the previous events he/she attended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "In sum, we propose a recommender system for event based social networks. We resolved the cold start problem associated with the event recommendations by using content based features: location, time, description and group frequency.\n",
    "\n",
    "Further, the next steps would be to use data from other major sites like eventbrite.com, facebook.com and evaluate our recommendation on it. Another work could be to use more features for the system, like: Multi-relational Bayesian personalized ranking. We can also perform learning to rank on our recommender using Rank-SVM or coordinate ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "[1] E. Minkov, B. Charrow, J. Ledlie, S. Teller, and T. Jaakkola, Collaborative future event recommendation, In Proc. of CIKM, pages 819-828, 2010.\n",
    "\n",
    "[2] Augusto Q. Macedo, Leandro B. Marinho and Rodrygo L. T. Santos, Context-Aware Event Recommendation in Event based Social Networks, In Proc. of Recsys, pages 123-130, 2015.\n",
    "\n",
    "[3] H. Khrouf and R. Troncy. Hybrid event recommendation using linked data and user diversity, In Proc. of RecSys, pages 185-192, 2013.\n",
    "\n",
    "[4] A. Q. Macedo and L. B. Marinho, Event recommendation in event-based social networks, In Proc. of Int. Work. on Social Personalization, 2014."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
