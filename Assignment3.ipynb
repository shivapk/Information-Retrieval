{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 1:  Basic Machine Learning + Learning to Rank \n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, February 12 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get hands-on experience with (i) the basics of machine learning (e.g. train/test data, cross-validation, different classifiers) and interpreting results; and (ii) learning to rank.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, February 15 at 11:59pm.\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basics of ML (70 points)\n",
    "\n",
    "For this part, we're going to get familiar with scikit-learn (a great ML toolkit that is very popular) and the major issues in training a model, testing it, and interpreting the results. Our goal in this assignment is to build a classifier to determine if a Yelp review is \"food-relevant\" or not.\n",
    "\n",
    "## Dataset: Yelp review data\n",
    "\n",
    "First, you will need to download the training_data.json file from the Resources tab on Piazza, a collection of 40,000 json-encoded Yelp reviews we sampled from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "You'll see that each line corresponds to a review on a particular business. The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Parsing Yelp (15 points)\n",
    "\n",
    "For this first part, we will build a parser for extracting tokens from the **review text** only. First, you should tokenize each review using **whitespaces and punctuations as delimiters**. Do not remove stopwords. You should apply casefolding (lower case everything) and use the [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) ... you may need to install nltk if you don't have it already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "import json\n",
    "import operator\n",
    "from sklearn.metrics import classification_report\n",
    "def clean_data(sentence):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokens=word_tokenize(sentence.lower())\n",
    "    result=[]\n",
    "    for word in tokens:    \n",
    "        stemmed_word=stemmer.stem(word)\n",
    "        if len(stemmed_word)<2:\n",
    "            continue     \n",
    "        result.append(stemmed_word)\n",
    "    return result\n",
    "\n",
    "def load_data():\n",
    "    labels=[]\n",
    "    file_path='training_data.json'\n",
    "    reviews=[]\n",
    "    with open(file_path) as fn:\n",
    "        for row in fn:\n",
    "            total_content = json.loads(row)\n",
    "            reviews.append(clean_data(total_content['text']))\n",
    "            if total_content['label']=='Food-irrelevant':\n",
    "                labels.append(0)\n",
    "            elif total_content['label']=='Food-relevant':\n",
    "                labels.append(1)\n",
    "    return (reviews,labels)\n",
    "    \n",
    "reviews,labels=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique tokens?\n",
    "\n",
    "Once you have your parser working, you should report here the size of your feature space. That is, how many unique tokens do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'fawn': 7,\n",
       " u'circuitri': 1,\n",
       " u'gai': 25,\n",
       " u'mid-week': 15,\n",
       " u'long-sleev': 1,\n",
       " u'woodi': 12,\n",
       " u'sonja': 1,\n",
       " u'*best*': 1,\n",
       " u'alphagraph': 2,\n",
       " u'suzann': 13,\n",
       " u'francesco': 1,\n",
       " u'francesca': 4,\n",
       " u'northsight': 1,\n",
       " u'nieder': 1,\n",
       " u'fawk': 1,\n",
       " u'frou-frou': 4,\n",
       " u'oval/natur': 1,\n",
       " u'aileen': 1,\n",
       " u'360.00': 1,\n",
       " u'scold': 19,\n",
       " u'la-wannab': 1,\n",
       " u'accret': 1,\n",
       " u'crespel': 2,\n",
       " u'mustachio': 2,\n",
       " u'wrought-iron': 1,\n",
       " u'everything.i': 1,\n",
       " u'half-tomato': 1,\n",
       " u'spagehetti': 2,\n",
       " u'course/vac': 1,\n",
       " u'foui': 1,\n",
       " u'calpico': 5,\n",
       " u'age/rac': 1,\n",
       " u'button-down': 3,\n",
       " u'sharehold': 2,\n",
       " u'wood-': 1,\n",
       " u'rude.sh': 1,\n",
       " u'pigment': 2,\n",
       " u'capoeira': 1,\n",
       " u'heard/sang': 1,\n",
       " u'thinned-down': 1,\n",
       " u'strictest': 1,\n",
       " u'viewabl': 2,\n",
       " u'elvi': 18,\n",
       " u'airfar': 1,\n",
       " u'broiler': 6,\n",
       " u'arizona..and': 1,\n",
       " u'wooden': 71,\n",
       " u'showcas': 46,\n",
       " u'understock': 1,\n",
       " u'wednesday': 274,\n",
       " u'*food': 2,\n",
       " u'tcbi': 9,\n",
       " u'high-and-dri': 1,\n",
       " u'crotch': 5,\n",
       " u'flair..': 1,\n",
       " u'redish': 1,\n",
       " u'cane-': 1,\n",
       " u'vermietst': 2,\n",
       " u'everything..': 3,\n",
       " u\"'thorn\": 1,\n",
       " u'tumer': 1,\n",
       " u'them-but': 1,\n",
       " u'bass-heavi': 1,\n",
       " u'27+': 2,\n",
       " u'1997.': 2,\n",
       " u'27.': 2,\n",
       " u'271': 2,\n",
       " u'sooth': 54,\n",
       " u'275': 1,\n",
       " u'sh1t': 1,\n",
       " u'spice/hot': 1,\n",
       " u'pleeeeeeaaaassseee': 1,\n",
       " u'dialogu': 5,\n",
       " u'1725.': 1,\n",
       " u'*actual*': 1,\n",
       " u'office-gray': 1,\n",
       " u'poor/extrem': 1,\n",
       " u'tapas-styl': 1,\n",
       " u'succumb': 6,\n",
       " u'visit-to-visit': 1,\n",
       " u'widget': 2,\n",
       " u'perfunctorili': 1,\n",
       " u'crouch': 2,\n",
       " u'dish..': 3,\n",
       " u'-then': 2,\n",
       " u'chasu-pork': 1,\n",
       " u'tiffiani': 1,\n",
       " u'karibu': 1,\n",
       " u'salad/sandwich': 2,\n",
       " u'friendlyish': 1,\n",
       " u'ching': 22,\n",
       " u'china': 265,\n",
       " u'74th': 1,\n",
       " u'wagyu': 5,\n",
       " u'doublecharg': 1,\n",
       " u'chino': 2,\n",
       " u'gent..wa': 1,\n",
       " u'-they': 19,\n",
       " u'from..everyon': 1,\n",
       " u'deterior': 17,\n",
       " u'naturel': 1,\n",
       " u'spotti': 51,\n",
       " u'80s-glitter': 1,\n",
       " u'over-zeal': 2,\n",
       " u'pastur': 2,\n",
       " u'kidd': 2,\n",
       " u'neurologist': 10,\n",
       " u'climber': 6,\n",
       " u'error/': 1,\n",
       " u'golden': 112,\n",
       " u'ass-kissish': 1,\n",
       " u'semiautomat': 1,\n",
       " u'lengthen': 2,\n",
       " u'orangish': 2,\n",
       " u'cranium': 1,\n",
       " u'prescenc': 1,\n",
       " u'sterl': 4,\n",
       " u'stern': 9,\n",
       " u'specs/materi': 1,\n",
       " u'westech': 1,\n",
       " u'dnd': 2,\n",
       " u'dna': 4,\n",
       " u'stubbornli': 2,\n",
       " u'phoenix/tempe/scottsdal': 1,\n",
       " u'reclean': 3,\n",
       " u'couch/b': 1,\n",
       " u'prospector': 1,\n",
       " u'dnt': 3,\n",
       " u'subcultur': 1,\n",
       " u'music': 1483,\n",
       " u'waitend': 1,\n",
       " u'catchi': 7,\n",
       " u'tables..th': 1,\n",
       " u'clock-th': 1,\n",
       " u'nebul': 1,\n",
       " u'butheru': 1,\n",
       " u'fast-ass': 1,\n",
       " u'multi-channel': 4,\n",
       " u'yahoo': 4,\n",
       " u'exce': 20,\n",
       " u'stock/': 1,\n",
       " u'5.15': 1,\n",
       " u'5.18': 1,\n",
       " u'brancott': 1,\n",
       " u'unpack': 6,\n",
       " u'benedryl': 1,\n",
       " u'exci': 1,\n",
       " u'time/review': 1,\n",
       " u'100.00': 13,\n",
       " u'spinning/clip-in': 1,\n",
       " u'palm-lin': 1,\n",
       " u'locker': 138,\n",
       " u'colorect': 2,\n",
       " u'half-ounc': 1,\n",
       " u'the.cutest': 1,\n",
       " u'lgo-er': 2,\n",
       " u'tomahawk': 1,\n",
       " u'packaging/gift': 1,\n",
       " u'select=9e2gzc41mkllyllhngcjig': 1,\n",
       " u'capsaicin': 1,\n",
       " u'wang': 8,\n",
       " u'wand': 10,\n",
       " u'etiquett': 22,\n",
       " u'anyhow..': 1,\n",
       " u'unjust': 1,\n",
       " u'xtra': 8,\n",
       " u'places..': 2,\n",
       " u'cookeri': 1,\n",
       " u'hand-shak': 1,\n",
       " u'117f': 1,\n",
       " u'bienvenu': 1,\n",
       " u'yummili': 1,\n",
       " u'pinta': 3,\n",
       " u'want': 8390,\n",
       " u'zieht': 1,\n",
       " u'delivery.a': 1,\n",
       " u'10-cours': 1,\n",
       " u'pinto': 12,\n",
       " u'pomegranit': 2,\n",
       " u'9/2/13': 1,\n",
       " u'brick-warehous': 1,\n",
       " u'mcmuffin': 3,\n",
       " u'sugarless': 1,\n",
       " u'vicent': 8,\n",
       " u'ex-mother-in-law': 1,\n",
       " u'travel': 747,\n",
       " u'copious': 1,\n",
       " u'cutback': 1,\n",
       " u'hough': 1,\n",
       " u'/day': 1,\n",
       " u'dangerfield': 1,\n",
       " u'urethra': 1,\n",
       " u'training-': 1,\n",
       " u'tchaikovski': 1,\n",
       " u'appeal/flavor/spic': 1,\n",
       " u'c/would': 1,\n",
       " u'caldo': 4,\n",
       " u'100-150': 1,\n",
       " u'goober': 1,\n",
       " u'hole-in-the-wal': 38,\n",
       " u'cooker-': 1,\n",
       " u'club-which': 1,\n",
       " u'schissler': 1,\n",
       " u'wrong': 1547,\n",
       " u'wristlet': 1,\n",
       " u'farrrr': 1,\n",
       " u'prodig': 1,\n",
       " u'lemon/orang': 1,\n",
       " u'matter-of-fact': 3,\n",
       " u'akira': 1,\n",
       " u'*but*': 2,\n",
       " u'pss-': 1,\n",
       " u\"'net\": 1,\n",
       " u\"'new\": 3,\n",
       " u'visit.and': 1,\n",
       " u'bordella': 1,\n",
       " u'18th': 13,\n",
       " u'photo-op': 2,\n",
       " u'lowlif': 1,\n",
       " u'disinclin': 1,\n",
       " u'4-': 7,\n",
       " u't_t': 1,\n",
       " u\"you'al\": 1,\n",
       " u'family-own': 25,\n",
       " u'natuzzi': 1,\n",
       " u'restrospect': 1,\n",
       " u'recombo': 1,\n",
       " u'tillamook': 1,\n",
       " u'snugli': 3,\n",
       " u'crave-': 1,\n",
       " u'pretti': 5669,\n",
       " u'wait-tim': 1,\n",
       " u'fir': 9,\n",
       " u'fip': 1,\n",
       " u'finisih': 1,\n",
       " u'fit': 1066,\n",
       " u'fix': 1483,\n",
       " u'-codi': 1,\n",
       " u'pickem-up': 1,\n",
       " u'cinna-con': 1,\n",
       " u'fib': 1,\n",
       " u'badself': 1,\n",
       " u'wickedli': 3,\n",
       " u'fig': 215,\n",
       " u'local-busi': 1,\n",
       " u'tee-off': 1,\n",
       " u\"'manag\": 2,\n",
       " u'fin': 87,\n",
       " u'fil': 1,\n",
       " u'fim': 1,\n",
       " u'to-that': 1,\n",
       " u'welcome-': 1,\n",
       " u'7qdcp4mv_ws2sdjtqmlvoa': 1,\n",
       " u'families/children': 1,\n",
       " u'satelit': 2,\n",
       " u'14.49': 1,\n",
       " u\"o'riva\": 1,\n",
       " u'ten-month-old': 1,\n",
       " u'r+b': 1,\n",
       " u'gnoochi': 1,\n",
       " u'sixteen': 8,\n",
       " u'nosh-': 1,\n",
       " u'lucera': 5,\n",
       " u'damnat': 1,\n",
       " u'arrog': 63,\n",
       " u'hemorrhag': 2,\n",
       " u'//bit.ly/clvlvg': 1,\n",
       " u'accommod': 533,\n",
       " u'close-ish': 1,\n",
       " u'arrow': 16,\n",
       " u'romantique..': 1,\n",
       " u'arroz': 29,\n",
       " u'christ-cent': 1,\n",
       " u'doorladi': 1,\n",
       " u'burial': 5,\n",
       " u'unshad': 1,\n",
       " u'allah': 1,\n",
       " u'allan': 5,\n",
       " u'green-leaf': 1,\n",
       " u'390': 1,\n",
       " u'ex-the-buy': 1,\n",
       " u'395': 1,\n",
       " u'worn-out': 2,\n",
       " u'allay': 4,\n",
       " u'thick-slic': 2,\n",
       " u'.gov': 1,\n",
       " u'hard..': 1,\n",
       " u'self-': 2,\n",
       " u'cripplingli': 1,\n",
       " u'beat-down': 1,\n",
       " u'oprah': 5,\n",
       " u'smirk': 11,\n",
       " u'mason': 17,\n",
       " u'ombr': 8,\n",
       " u'20indian': 1,\n",
       " u'adapt': 27,\n",
       " u'\\xfcberfl\\xfcssig': 1,\n",
       " u'cassava': 1,\n",
       " u'outburst': 2,\n",
       " u'stew-lik': 4,\n",
       " u'allpet': 1,\n",
       " u'3749': 1,\n",
       " u'gameplan': 2,\n",
       " u'renegoti': 1,\n",
       " u'mattter': 1,\n",
       " u'.exactli': 1,\n",
       " u'converstaion': 1,\n",
       " u'deal-break': 5,\n",
       " u'petul': 1,\n",
       " u'stratu': 3,\n",
       " u'leele': 2,\n",
       " u'gameplay': 1,\n",
       " u'leela': 2,\n",
       " u'chicago-them': 1,\n",
       " u'competet': 2,\n",
       " u\"n'sync\": 1,\n",
       " u'chiara': 1,\n",
       " u'v.i.p': 1,\n",
       " u'barrelhous': 1,\n",
       " u'momment': 1,\n",
       " u'sickeningli': 5,\n",
       " u'man-mad': 2,\n",
       " u'yester': 1,\n",
       " u'2-time': 1,\n",
       " u'ardour': 1,\n",
       " u'invigor': 3,\n",
       " u'awesommmm': 1,\n",
       " u\"'possum\": 1,\n",
       " u'errrth': 1,\n",
       " u'sentra': 1,\n",
       " u'mesmer': 7,\n",
       " u'refust': 1,\n",
       " u'injector': 5,\n",
       " u'clementin': 2,\n",
       " u'kfc': 18,\n",
       " u'chevron': 6,\n",
       " u'biglot': 2,\n",
       " u'viett': 1,\n",
       " u'pronghorn': 1,\n",
       " u'looov': 7,\n",
       " u'roundabout': 1,\n",
       " u'methun': 2,\n",
       " u'~the': 15,\n",
       " u'17+tip': 1,\n",
       " u'.clearli': 1,\n",
       " u'payless': 6,\n",
       " u'fishfri': 1,\n",
       " u'reuben': 90,\n",
       " u\"shampoo'\": 2,\n",
       " u'master': 157,\n",
       " u'stores..': 1,\n",
       " u'oldi': 15,\n",
       " u'critter': 34,\n",
       " u'roadhous': 17,\n",
       " u'understaf': 59,\n",
       " u'fili-b': 1,\n",
       " u'61.92': 1,\n",
       " u'rakim': 1,\n",
       " u'-too': 3,\n",
       " u'-tom': 1,\n",
       " u'scraper': 3,\n",
       " u'doreen': 2,\n",
       " u'9-until': 1,\n",
       " u'people.so': 1,\n",
       " u'mouth-feel': 2,\n",
       " u'ampl': 120,\n",
       " u'fondl': 1,\n",
       " u'tohono': 1,\n",
       " u'non-beer': 3,\n",
       " u'restaurant/cafeteria': 1,\n",
       " u'sheen': 1,\n",
       " u'bellpepp': 1,\n",
       " u'old-': 1,\n",
       " u'seizure.': 1,\n",
       " u'instructors/employe': 1,\n",
       " u'uniniti': 4,\n",
       " u'sausage-brat-hotdog': 1,\n",
       " u'brawni': 1,\n",
       " u'flavors.lastli': 1,\n",
       " u'spectrum': 27,\n",
       " u'egg-chedcheese-onion-ham': 1,\n",
       " u'by-the-slic': 1,\n",
       " u'increment': 3,\n",
       " u'pomp': 3,\n",
       " u'belly-danc': 2,\n",
       " u'.sinc': 3,\n",
       " u'dozen': 256,\n",
       " u'2.79': 1,\n",
       " u'.bizarr': 1,\n",
       " u're-check': 1,\n",
       " u'dentist/or': 1,\n",
       " u'over-enthusiast': 1,\n",
       " u'bisqu': 132,\n",
       " u'uncouth': 2,\n",
       " u'2.75': 20,\n",
       " u'shoppabl': 1,\n",
       " u'slaw..': 1,\n",
       " u'sh*t.': 3,\n",
       " u'-wild': 1,\n",
       " u'recommended-': 1,\n",
       " u'simplifi': 6,\n",
       " u'spanish-villa': 1,\n",
       " u'ghostown': 1,\n",
       " u'canist': 4,\n",
       " u'unknow': 3,\n",
       " u'limitless': 5,\n",
       " u'n.phoenix': 1,\n",
       " u'toil': 5,\n",
       " u'mouth': 557,\n",
       " u'monger': 1,\n",
       " u'fluster': 12,\n",
       " u'necessary/author': 1,\n",
       " u'nurse/p.a': 1,\n",
       " u'.litt': 1,\n",
       " u'weine': 1,\n",
       " u'cheap.th': 1,\n",
       " u'singer': 37,\n",
       " u'4,470': 1,\n",
       " u'unpleas': 80,\n",
       " u'10:30am': 8,\n",
       " u'everythin': 1,\n",
       " u'cumbia': 1,\n",
       " u'schlendert': 1,\n",
       " u'tech': 459,\n",
       " u'phone-lik': 1,\n",
       " u'hiatu': 8,\n",
       " u'hiatt': 1,\n",
       " u'bisexu': 1,\n",
       " u'scream': 217,\n",
       " u'well-belov': 1,\n",
       " u'cemita': 1,\n",
       " u'20/15': 2,\n",
       " u'14-18': 1,\n",
       " u'20/10': 1,\n",
       " u'post-work': 4,\n",
       " u'sweetner': 2,\n",
       " u'fast-food-chain': 1,\n",
       " u'dicken': 2,\n",
       " u\"'breakfast\": 1,\n",
       " u'fennel': 23,\n",
       " u'select=aykzwja1xsgxlobawbwhog': 1,\n",
       " u'traffic-fre': 1,\n",
       " u'joyou': 2,\n",
       " u'hi-tech': 2,\n",
       " u'extemporan': 2,\n",
       " u'restaut': 1,\n",
       " u'apach': 33,\n",
       " u'restaur': 5866,\n",
       " u'rico': 14,\n",
       " u'lube': 87,\n",
       " u'bliss': 43,\n",
       " u'rick': 26,\n",
       " u'rich': 330,\n",
       " u'yyyeeaa~uck': 1,\n",
       " u'rice': 1704,\n",
       " u'lubi': 3,\n",
       " u'rica': 3,\n",
       " u'plate': 1884,\n",
       " u'reflar': 1,\n",
       " u'too-cool': 1,\n",
       " u'waaaaaay': 5,\n",
       " u'plato': 8,\n",
       " u'7-13': 1,\n",
       " u'2min': 1,\n",
       " u'7-11': 7,\n",
       " u'ceil': 202,\n",
       " u'platt': 3,\n",
       " u\"o'rielli\": 2,\n",
       " u'.instant': 1,\n",
       " u'waaaaaah': 1,\n",
       " u'platz': 3,\n",
       " u'3:07.': 2,\n",
       " u'ahhh-maaaazzz': 1,\n",
       " u'6:10am': 1,\n",
       " u'jaguar': 20,\n",
       " u'backkkkk': 1,\n",
       " u'rectal': 1,\n",
       " u'pregnancy/matern': 1,\n",
       " u'suffragett': 1,\n",
       " u'boarder': 5,\n",
       " u'non-perish': 2,\n",
       " u'*cross': 1,\n",
       " u'pretzel': 68,\n",
       " u'patch': 77,\n",
       " u'1-top': 2,\n",
       " u'anti-food': 1,\n",
       " u'affordabiltiy': 1,\n",
       " u'blue-cheese-potato': 1,\n",
       " u'etim': 1,\n",
       " u'top-most': 1,\n",
       " u'.in': 6,\n",
       " u'insomniacian': 1,\n",
       " u'.im': 1,\n",
       " u'potato-i': 1,\n",
       " u'.if': 24,\n",
       " u'helicoil': 1,\n",
       " u'..he': 3,\n",
       " u'pinot': 40,\n",
       " u'toy/stuf': 1,\n",
       " u'pre-diabet': 1,\n",
       " u'erika': 2,\n",
       " u'3,456': 1,\n",
       " u'smp': 1,\n",
       " u'.it': 74,\n",
       " u'dishwash': 21,\n",
       " u'fries-boo': 1,\n",
       " u'48th': 22,\n",
       " u'nachfrag': 1,\n",
       " u'heterogen': 1,\n",
       " u'corneliu': 1,\n",
       " u'irr': 1,\n",
       " u'lotu': 7,\n",
       " u'iri': 3,\n",
       " u'irk': 13,\n",
       " u'advisedli': 1,\n",
       " u\"'great\": 1,\n",
       " u'ira': 2,\n",
       " u'8:50pm': 1,\n",
       " u'frambois': 1,\n",
       " u'padrez': 2,\n",
       " u'top-flight': 1,\n",
       " u'rundown': 26,\n",
       " u'jetblu': 1,\n",
       " u'extend': 144,\n",
       " u'rude/clueless/slow': 1,\n",
       " u'ostercamp': 1,\n",
       " u'sweet/salti': 1,\n",
       " u'extens': 237,\n",
       " u'extent': 26,\n",
       " u'wheelbarrow': 1,\n",
       " u'forty-f': 3,\n",
       " u'8p-12a': 1,\n",
       " u'airflow': 3,\n",
       " u'veer': 12,\n",
       " u'53718.': 1,\n",
       " u'unemploy': 8,\n",
       " u'veez': 1,\n",
       " u'pole-d': 1,\n",
       " u'1/09': 1,\n",
       " u'we-rent': 1,\n",
       " u'lookin': 29,\n",
       " u'himalayan': 1,\n",
       " u'ariel': 3,\n",
       " u\"'send\": 1,\n",
       " u'lowrey': 1,\n",
       " u'assur': 217,\n",
       " u'buffett-own': 1,\n",
       " u'tmnt': 1,\n",
       " u'gopher': 2,\n",
       " u'pre-heaven': 1,\n",
       " u'blondi': 9,\n",
       " u'month-long': 4,\n",
       " u'ozium': 1,\n",
       " u'assum': 315,\n",
       " u'buttload': 1,\n",
       " u'lanett': 2,\n",
       " u'fra': 2,\n",
       " u'3.80': 1,\n",
       " u'sympama': 1,\n",
       " u'cuf': 2,\n",
       " u'union': 80,\n",
       " u'back-pretti': 1,\n",
       " u'fri': 3447,\n",
       " u'fro': 8,\n",
       " u'mucu': 1,\n",
       " u'meatballs/spinach/mushroom': 1,\n",
       " u'muck': 3,\n",
       " u'much': 6366,\n",
       " u\"let's-open-a-restaurant-in-an-old-hous\": 1,\n",
       " u'bbq-': 1,\n",
       " u'fun..': 4,\n",
       " u'devers': 1,\n",
       " u'tallest': 4,\n",
       " u'cheuvront-': 1,\n",
       " u'distast': 1,\n",
       " u'regul': 23,\n",
       " u'retrospect': 12,\n",
       " u'spit': 56,\n",
       " u'japanese/italian': 1,\n",
       " u'needed-': 2,\n",
       " u'call-in/internet': 1,\n",
       " u'dave': 125,\n",
       " u'spif': 3,\n",
       " u'stepdad': 1,\n",
       " u'spin': 178,\n",
       " u'bar/cellar': 1,\n",
       " u'davi': 11,\n",
       " u'wildcat': 2,\n",
       " u'1677': 1,\n",
       " u'game.on': 1,\n",
       " u'makenzi': 1,\n",
       " u'intak': 23,\n",
       " u'up..they': 1,\n",
       " u'employ': 92,\n",
       " u'droppin': 1,\n",
       " u'overth': 1,\n",
       " u'build-it-yer-self': 1,\n",
       " u'real-deal': 5,\n",
       " u'applese': 2,\n",
       " u'ever-so-slightli': 1,\n",
       " u'grey/blu': 1,\n",
       " u\"cookin'\": 1,\n",
       " u'angioplasti': 2,\n",
       " u'ala-cart': 1,\n",
       " u'restaurant/shop': 1,\n",
       " u'offish': 3,\n",
       " u'expat': 4,\n",
       " u'kohn': 1,\n",
       " u'vietnemes': 1,\n",
       " u'gentrifi': 6,\n",
       " u'expecting/hop': 1,\n",
       " u'mirabella': 1,\n",
       " u'eighteen': 5,\n",
       " u'honeslti': 3,\n",
       " u'transplat': 1,\n",
       " u'wizrd': 1,\n",
       " u'oxymoron': 5,\n",
       " u'hone': 4,\n",
       " u'hong': 61,\n",
       " u'recover': 1,\n",
       " u'dilut': 9,\n",
       " u'that-th': 1,\n",
       " u'indescrib': 4,\n",
       " u'overcooked..': 1,\n",
       " u'dazzl': 13,\n",
       " u'split': 412,\n",
       " u'jungl': 38,\n",
       " u'briney': 2,\n",
       " u'.atroci': 1,\n",
       " u'***repli': 1,\n",
       " u'8/24': 1,\n",
       " u'issac': 4,\n",
       " u'frenchmen': 2,\n",
       " u'refir': 1,\n",
       " u'refit': 1,\n",
       " u'qdoba': 22,\n",
       " u\"'stole\": 1,\n",
       " u'..thumb': 2,\n",
       " u'refil': 468,\n",
       " u'supper': 53,\n",
       " u\"slice's/slic\": 1,\n",
       " u'southi': 2,\n",
       " u'cask-condit': 2,\n",
       " u'nibbl': 29,\n",
       " u'anniversary.w': 1,\n",
       " u'furlough': 1,\n",
       " u'appetizer..': 1,\n",
       " u'w/french': 1,\n",
       " u'molehil': 2,\n",
       " u'everon': 1,\n",
       " u\"'cheesecak\": 1,\n",
       " u'okay..it': 1,\n",
       " u'rey': 3,\n",
       " u'french/southwestern': 1,\n",
       " u'knic-knacki': 1,\n",
       " u'controversi': 4,\n",
       " u'bellow': 4,\n",
       " u'buffyvers': 4,\n",
       " u'dtap': 1,\n",
       " u'bleck': 1,\n",
       " u'r\\xfcckgabe': 2,\n",
       " u'waterboard': 1,\n",
       " u'stni': 3,\n",
       " u'albondiga': 14,\n",
       " u'lassi': 9,\n",
       " u'brandish': 2,\n",
       " u'mercycar': 2,\n",
       " u'hah': 10,\n",
       " u'hai': 6,\n",
       " u'ham': 226,\n",
       " u'quaint-littl': 1,\n",
       " u'berni': 1,\n",
       " u'out-of-town': 61,\n",
       " u'haa': 2,\n",
       " u'hab': 1,\n",
       " u'had': 27710,\n",
       " u'nosheri': 1,\n",
       " u'stuff-like-that': 1,\n",
       " u'godlik': 1,\n",
       " u'hay': 12,\n",
       " u'purse-said': 1,\n",
       " u'quart-siz': 1,\n",
       " u'endur': 57,\n",
       " u'hap': 9,\n",
       " u'daaaamnnnnnn': 1,\n",
       " u'har': 23,\n",
       " u'hat': 161,\n",
       " u'hau': 4,\n",
       " u'saturated..': 1,\n",
       " u'haw': 8,\n",
       " u'otherworldli': 2,\n",
       " u'steel-tip': 2,\n",
       " u'leeandr': 1,\n",
       " u'confection': 1,\n",
       " u'implor': 3,\n",
       " u'sacreligi': 1,\n",
       " u'pseudo-warehous': 1,\n",
       " u'mexican/latin': 2,\n",
       " u'yawnfest': 1,\n",
       " u'fast-talk': 1,\n",
       " u'shadow': 35,\n",
       " u'12:30': 24,\n",
       " u'semi-drop': 1,\n",
       " u'forrrevvvv': 1,\n",
       " u\"'summer\": 1,\n",
       " u'2-3.': 2,\n",
       " u'over-bread': 2,\n",
       " u'hilton-run': 1,\n",
       " u'delieveri': 1,\n",
       " u'house.bruno': 1,\n",
       " u'gadz': 2,\n",
       " u'..wrong': 1,\n",
       " u'ionic': 1,\n",
       " u'celebr8ion': 1,\n",
       " u'profit-bas': 1,\n",
       " u'tombston': 2,\n",
       " u'attorney': 31,\n",
       " u'suddemli': 1,\n",
       " u'crowd': 1573,\n",
       " u'czech': 3,\n",
       " u'pickguard': 1,\n",
       " u'crown': 93,\n",
       " u'mock-antiqu': 1,\n",
       " u'unabashedli': 2,\n",
       " u'anti-acid': 1,\n",
       " u'kick..it': 1,\n",
       " u'billboard': 19,\n",
       " u'hot/cold': 4,\n",
       " u'52nd': 3,\n",
       " u'botton': 1,\n",
       " u'bottom': 418,\n",
       " u'plantlif': 1,\n",
       " u'lockdown': 1,\n",
       " u'adjustor': 2,\n",
       " u'inhuman': 2,\n",
       " u'completli': 4,\n",
       " u'cholula/tabasco': 1,\n",
       " u'wirklichkeit': 1,\n",
       " u'8yr': 1,\n",
       " u'is-mi': 1,\n",
       " u'before.nev': 1,\n",
       " u'beyond..': 2,\n",
       " u'wayfair': 1,\n",
       " u'binder': 12,\n",
       " u'heat-strok': 1,\n",
       " u'squat/deadlift': 1,\n",
       " u'antechamb': 1,\n",
       " u'select=n0burnlecyrkudc43fjnmq': 1,\n",
       " u'prices/valu': 1,\n",
       " u'visitng': 1,\n",
       " u'rodrigo': 1,\n",
       " u'aboard': 2,\n",
       " u'pavililion': 1,\n",
       " u'fisi': 1,\n",
       " u'musiq': 1,\n",
       " u'lugguag': 1,\n",
       " u'sequin': 6,\n",
       " u'//www.silvereaglebar.net/': 1,\n",
       " u'amaze-ballz': 1,\n",
       " u'ricardson': 1,\n",
       " u'disgusting-and': 1,\n",
       " u'hand.': 1,\n",
       " u'coconut-lim': 1,\n",
       " u'honeymoom': 1,\n",
       " u'creatin': 1,\n",
       " u'dyson': 10,\n",
       " u'honeymoon': 15,\n",
       " u'mba': 3,\n",
       " u'unforun': 1,\n",
       " u'creativ': 186,\n",
       " u'swelter': 19,\n",
       " u'mbp': 2,\n",
       " u'budget/diy': 1,\n",
       " u'mbr': 1,\n",
       " u'saucier': 1,\n",
       " u'fabric': 227,\n",
       " u'suffici': 35,\n",
       " u'lastest': 2,\n",
       " u'veg-head': 2,\n",
       " u'tame': 18,\n",
       " u'erkert': 1,\n",
       " u'kids-for-the-day': 1,\n",
       " u'britischen': 1,\n",
       " u'raper': 1,\n",
       " u'tamp': 1,\n",
       " u'hour-glass': 1,\n",
       " u'.cozi': 1,\n",
       " u'cerignola': 1,\n",
       " u'headach': 74,\n",
       " u'welllllllll': 1,\n",
       " u'cordonado': 1,\n",
       " u'raceway': 8,\n",
       " u'peperoni': 2,\n",
       " u'bridgework': 1,\n",
       " u'work-rel': 1,\n",
       " u'duel': 4,\n",
       " u'amerikanischen': 1,\n",
       " u'polsc': 1,\n",
       " u'lockerroom': 1,\n",
       " u\"n'awlin\": 2,\n",
       " u'layin': 1,\n",
       " u'largish': 1,\n",
       " u'duec': 2,\n",
       " u'break-up': 1,\n",
       " u'azz': 3,\n",
       " u'open-air': 3,\n",
       " u'nicest': 127,\n",
       " u'kelbasa': 1,\n",
       " u'duet': 4,\n",
       " u'creuset': 4,\n",
       " u'friday1/4/13': 1,\n",
       " u'minibar': 1,\n",
       " u'eat-inn': 1,\n",
       " u'salon..': 1,\n",
       " u'fire-roast': 4,\n",
       " u'bourdain': 5,\n",
       " u'bedsheet': 3,\n",
       " u'1.30pm': 1,\n",
       " u'diput': 1,\n",
       " u'minds-': 1,\n",
       " u'wordlessli': 1,\n",
       " u'pasadena': 8,\n",
       " u'role': 28,\n",
       " u'minefield': 1,\n",
       " u'talb': 2,\n",
       " u'rolf': 5,\n",
       " u'roli': 1,\n",
       " u'macaroon': 16,\n",
       " u'roll': 1630,\n",
       " u'rolo': 1,\n",
       " u'mp5': 1,\n",
       " u'school/work': 1,\n",
       " u'flavortown': 1,\n",
       " u'intend': 112,\n",
       " u'ear-split': 1,\n",
       " u'.move': 1,\n",
       " u'devot': 32,\n",
       " u'avma': 2,\n",
       " u'center': 1176,\n",
       " u'calzone-': 1,\n",
       " u'dansko': 1,\n",
       " u'marisa': 2,\n",
       " u'reaonabl': 1,\n",
       " u'intens': 92,\n",
       " u'intent': 119,\n",
       " u'hawkey': 1,\n",
       " u'tramad': 1,\n",
       " u'sweet-tast': 1,\n",
       " u'work/school': 1,\n",
       " u'deshuesado-': 1,\n",
       " u'loren': 3,\n",
       " u'ingredients-lik': 1,\n",
       " u'602-694-2244': 1,\n",
       " u'guy..': 1,\n",
       " u'biomechan': 1,\n",
       " u'devonshir': 2,\n",
       " u'geometr': 2,\n",
       " u'restaurants..': 2,\n",
       " u'5.16': 1,\n",
       " u'grape/vin': 1,\n",
       " u'pyrotechn': 1,\n",
       " u'about..to': 1,\n",
       " u'ill-fit': 3,\n",
       " u'gown': 58,\n",
       " u'140-acres-': 1,\n",
       " u'cincinnati': 5,\n",
       " u'chain': 806,\n",
       " u'whoever': 80,\n",
       " u'osp': 1,\n",
       " u'osu': 3,\n",
       " u'bar/restaurant/mus': 1,\n",
       " u'vasqu': 1,\n",
       " u'rope-esk': 1,\n",
       " u'oso': 3,\n",
       " u'46f': 1,\n",
       " u'chair': 803,\n",
       " u'chais': 1,\n",
       " u'french-styl': 3,\n",
       " u'bewild': 9,\n",
       " u'osf': 31,\n",
       " u'machu': 1,\n",
       " u'ballet': 66,\n",
       " u'catalyt': 11,\n",
       " u'baller': 5,\n",
       " u'teehe': 1,\n",
       " u'ni\\xe7ois': 2,\n",
       " u'indivu': 1,\n",
       " u'espaniol': 1,\n",
       " u'dicki': 1,\n",
       " u'annoying-': 1,\n",
       " u'e-collar': 3,\n",
       " u'zeppoli': 2,\n",
       " u'macha': 2,\n",
       " u'muy-choi-kow-yuk': 1,\n",
       " u'downtim': 5,\n",
       " u'macho': 4,\n",
       " u'oversight': 8,\n",
       " u'//promotagged.com/': 1,\n",
       " u'over-the-top': 14,\n",
       " u'gloomi': 4,\n",
       " u'jeri': 1,\n",
       " u'chai-': 1,\n",
       " u'jerk': 111,\n",
       " u'urbanesqu': 1,\n",
       " u'lit-up': 2,\n",
       " u'indie-kid': 1,\n",
       " u'hardwork': 9,\n",
       " u'exager': 4,\n",
       " u'embark': 5,\n",
       " u'corp.': 2,\n",
       " u'exact': 210,\n",
       " u'child-': 1,\n",
       " u'undeni': 5,\n",
       " u'dishes-for': 1,\n",
       " u'defaulti': 1,\n",
       " u'6:30-ish': 1,\n",
       " u'picur': 2,\n",
       " u'15-20min': 1,\n",
       " u'udong': 1,\n",
       " u\"wally'\": 2,\n",
       " u'whhhhaaaat': 1,\n",
       " u'cooki': 643,\n",
       " u'skewer': 76,\n",
       " u'1.49.': 1,\n",
       " u'homebodi': 1,\n",
       " u'meadow': 1,\n",
       " u'motelform': 1,\n",
       " u'invinc': 1,\n",
       " u'raquetbal': 2,\n",
       " u'11:45': 12,\n",
       " u'heavyweight': 2,\n",
       " u'buut': 1,\n",
       " u'11:40': 1,\n",
       " u'panko-crust': 1,\n",
       " u\"'back\": 1,\n",
       " u'supermarket..th': 1,\n",
       " u'cash.oh': 1,\n",
       " u'headset': 11,\n",
       " u'nonjudgment': 1,\n",
       " u'hesist': 1,\n",
       " u'chimichanga': 102,\n",
       " u'cmft': 1,\n",
       " u'upwardly-lounge-casu': 1,\n",
       " u'burch': 2,\n",
       " u'dreamy-creami': 1,\n",
       " u'wove': 1,\n",
       " u'wayward': 2,\n",
       " u'douchesquid': 1,\n",
       " u'kryptonit': 4,\n",
       " u'shrimp/prawn': 1,\n",
       " u'cassi': 1,\n",
       " u'boost': 32,\n",
       " u'dust/': 1,\n",
       " u'303': 3,\n",
       " u'2-4.': 1,\n",
       " u'honour': 3,\n",
       " u'you-cqll-it': 1,\n",
       " u'prices/qu': 1,\n",
       " u'non-cajun': 1,\n",
       " u'door-fram': 1,\n",
       " u'modif': 18,\n",
       " u'address': 240,\n",
       " u'say-': 2,\n",
       " u'ratio': 56,\n",
       " u'rework': 5,\n",
       " u'mafioso': 1,\n",
       " u'gnar': 1,\n",
       " u'fledgl': 1,\n",
       " u'red-ink': 1,\n",
       " u'cusack': 1,\n",
       " u'timbuktu': 2,\n",
       " u'patienc': 105,\n",
       " u'cartrail': 1,\n",
       " u'115.00': 1,\n",
       " u'dusti': 36,\n",
       " u'easygo': 4,\n",
       " u'brewin': 1,\n",
       " u'lucero': 1,\n",
       " u'influx': 3,\n",
       " u'four-months-out-of-warranti': 1,\n",
       " u'type-': 1,\n",
       " u'housemad': 16,\n",
       " u'ysl': 6,\n",
       " u'cock-a-poo': 1,\n",
       " u'//www.fcc-phx.com/': 1,\n",
       " u'pbbbbbbbbbt': 1,\n",
       " u'cornbeef': 1,\n",
       " u'instal': 516,\n",
       " u'cacophoni': 3,\n",
       " u'umph': 2,\n",
       " u'be..': 7,\n",
       " u'alter..etc': 1,\n",
       " u'lighthous': 2,\n",
       " u'*fresh*': 1,\n",
       " u'luthier': 3,\n",
       " u'hyatt': 18,\n",
       " u'-fortun': 1,\n",
       " u'w/ice': 1,\n",
       " u'photography/video': 1,\n",
       " u'bay-be': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_vocab(reviews):\n",
    "    vocab={}\n",
    "    for row in reviews:\n",
    "        for word in row:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word]+=1\n",
    "    return vocab\n",
    "        \n",
    "#reviews=load_data()[0]\n",
    "vocab=create_vocab(reviews)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Most Popular Words\n",
    "\n",
    "Great, now we can tokenize the documents. Let's make a list of the most popular words in our reviews. For this step, you should maintain a count of how many times each word occurs. Then you should print out the top-20 words in your reviews.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "Rank Token Count\n",
    "\n",
    "1 awesome 78\n",
    "\n",
    "... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 245729),\n",
       " (u'and', 168032),\n",
       " (u'to', 127570),\n",
       " (u'it', 78529),\n",
       " (u'wa', 77154),\n",
       " (u'of', 76024),\n",
       " (u'is', 65155),\n",
       " (u'for', 60780),\n",
       " (u'in', 59689),\n",
       " (u'that', 50726),\n",
       " (u'my', 50497),\n",
       " (u'you', 45700),\n",
       " (u'they', 43485),\n",
       " (u'have', 40085),\n",
       " (u'thi', 39808),\n",
       " (u'with', 39303),\n",
       " (u'but', 37754),\n",
       " (u'on', 35108),\n",
       " (u\"'s\", 34320),\n",
       " (u\"n't\", 33615),\n",
       " (u'we', 31528),\n",
       " (u'not', 30354),\n",
       " (u'are', 27791),\n",
       " (u'had', 27710),\n",
       " (u'at', 26684),\n",
       " (u'so', 25609),\n",
       " (u'be', 25362),\n",
       " (u'place', 24930),\n",
       " (u'me', 23555),\n",
       " (u'there', 22897),\n",
       " (u'as', 21171),\n",
       " (u'were', 21058),\n",
       " (u'good', 20586),\n",
       " (u'do', 19921),\n",
       " (u'get', 19490),\n",
       " (u'time', 19295),\n",
       " (u'go', 19288),\n",
       " (u'like', 19129),\n",
       " (u'...', 18677),\n",
       " (u'if', 18495),\n",
       " (u'great', 17841),\n",
       " (u'all', 17671),\n",
       " (u'out', 17670),\n",
       " (u'here', 17394),\n",
       " (u'food', 17365),\n",
       " (u'veri', 16476),\n",
       " (u'one', 16284),\n",
       " (u'just', 16261),\n",
       " (u'when', 15036),\n",
       " (u'or', 15035),\n",
       " (u'from', 14754),\n",
       " (u'would', 14409),\n",
       " (u'their', 14196),\n",
       " (u'up', 14031),\n",
       " (u'about', 13508),\n",
       " (u'an', 13252),\n",
       " (u'servic', 13170),\n",
       " (u'did', 13020),\n",
       " (u'been', 12781),\n",
       " (u'what', 12440),\n",
       " (u'back', 12415),\n",
       " (u'he', 11869),\n",
       " (u'some', 11837),\n",
       " (u'realli', 11761),\n",
       " (u'can', 11695),\n",
       " (u'our', 11560),\n",
       " (u'your', 11421),\n",
       " (u\"''\", 11320),\n",
       " (u'will', 11279),\n",
       " (u'``', 10988),\n",
       " (u'order', 10556),\n",
       " (u'no', 10521),\n",
       " (u'love', 10182),\n",
       " (u'she', 9969),\n",
       " (u'them', 9861),\n",
       " (u'which', 9839),\n",
       " (u\"'ve\", 9735),\n",
       " (u'onli', 9731),\n",
       " (u'look', 9709),\n",
       " (u'more', 9609),\n",
       " (u'other', 9575),\n",
       " (u'even', 9512),\n",
       " (u'by', 9446),\n",
       " (u'tri', 9411),\n",
       " (u'ha', 8970),\n",
       " (u'also', 8904),\n",
       " (u'alway', 8701),\n",
       " (u'make', 8634),\n",
       " (u'becaus', 8543),\n",
       " (u'could', 8424),\n",
       " (u'nice', 8398),\n",
       " (u'want', 8390),\n",
       " (u\"'m\", 8288),\n",
       " (u'price', 8144),\n",
       " (u'come', 8076),\n",
       " (u'need', 7870),\n",
       " (u'after', 7777),\n",
       " (u'well', 7696),\n",
       " (u'got', 7627),\n",
       " (u'know', 7531),\n",
       " (u'day', 7504),\n",
       " (u'than', 7468),\n",
       " (u'work', 7413),\n",
       " (u'littl', 7308),\n",
       " (u'too', 7266),\n",
       " (u'best', 7253),\n",
       " (u'take', 7042),\n",
       " (u'us', 6992),\n",
       " (u'wait', 6925),\n",
       " (u'never', 6921),\n",
       " (u'staff', 6901),\n",
       " (u'year', 6745),\n",
       " (u'peopl', 6691),\n",
       " (u'went', 6642),\n",
       " (u'thing', 6641),\n",
       " (u'over', 6566),\n",
       " (u'who', 6418),\n",
       " (u'much', 6366),\n",
       " (u'friendli', 6276),\n",
       " (u'first', 6168),\n",
       " (u'say', 6153),\n",
       " (u'then', 6094),\n",
       " (u'ask', 6033),\n",
       " (u'her', 5951),\n",
       " (u'restaur', 5866),\n",
       " (u'store', 5854),\n",
       " (u'think', 5828),\n",
       " (u'see', 5816),\n",
       " (u'am', 5708),\n",
       " (u'help', 5692),\n",
       " (u'lot', 5679),\n",
       " (u'pretti', 5669),\n",
       " (u'call', 5643),\n",
       " (u'way', 5605),\n",
       " (u'use', 5574),\n",
       " (u'off', 5554),\n",
       " (u'how', 5538),\n",
       " (u'ani', 5470),\n",
       " (u'find', 5446),\n",
       " (u'now', 5280),\n",
       " (u'new', 5236),\n",
       " (u'two', 5223),\n",
       " (u'right', 5142),\n",
       " (u'experi', 5117),\n",
       " (u'said', 5076),\n",
       " (u'few', 5061),\n",
       " (u'phoenix', 5025),\n",
       " (u'car', 5015),\n",
       " (u'befor', 4988),\n",
       " (u'better', 4956),\n",
       " (u'locat', 4950),\n",
       " (u'bar', 4948),\n",
       " (u'again', 4936),\n",
       " (u'around', 4929),\n",
       " (u'give', 4876),\n",
       " (u'came', 4803),\n",
       " (u'hour', 4777),\n",
       " (u'still', 4766),\n",
       " (u\"'re\", 4762),\n",
       " (u'night', 4752),\n",
       " (u'down', 4655),\n",
       " (u'feel', 4630),\n",
       " (u'room', 4596),\n",
       " (u'chicken', 4594),\n",
       " (u'made', 4590),\n",
       " (u'friend', 4589),\n",
       " (u'sinc', 4571),\n",
       " (u'eat', 4567),\n",
       " (u'review', 4562),\n",
       " (u'sure', 4557),\n",
       " (u'drink', 4543),\n",
       " (u'custom', 4540),\n",
       " (u'seem', 4532),\n",
       " (u'walk', 4515),\n",
       " (u'busi', 4507),\n",
       " (u'minut', 4480),\n",
       " (u'menu', 4463),\n",
       " (u'while', 4462),\n",
       " (u'recommend', 4443),\n",
       " (u'area', 4374),\n",
       " (u'star', 4363),\n",
       " (u'ever', 4359),\n",
       " (u'lunch', 4345),\n",
       " (u'everi', 4343),\n",
       " (u'most', 4338),\n",
       " (u'into', 4189),\n",
       " (u'hi', 4159),\n",
       " (u\"'ll\", 4148),\n",
       " (u'visit', 4129),\n",
       " (u'shop', 4117),\n",
       " (u'mani', 4106),\n",
       " (u'check', 4088),\n",
       " (u'someth', 4084),\n",
       " (u'told', 4082),\n",
       " (u'last', 4048),\n",
       " (u'where', 4014),\n",
       " (u'park', 3948),\n",
       " (u'anoth', 3940),\n",
       " (u'clean', 3936),\n",
       " (u'sandwich', 3912),\n",
       " (u'next', 3909),\n",
       " (u'took', 3891),\n",
       " (u'definit', 3880),\n",
       " (u'tabl', 3867),\n",
       " (u'enjoy', 3828),\n",
       " (u'home', 3791),\n",
       " (u'ca', 3787),\n",
       " (u'everyth', 3768),\n",
       " (u'amaz', 3727),\n",
       " (u'chees', 3727),\n",
       " (u'bad', 3721),\n",
       " (u'salad', 3686),\n",
       " (u'doe', 3669),\n",
       " (u'though', 3636),\n",
       " (u'delici', 3564),\n",
       " (u'big', 3496),\n",
       " (u'bit', 3487),\n",
       " (u'tast', 3471),\n",
       " (u'happi', 3448),\n",
       " (u'fri', 3447),\n",
       " (u'sauc', 3386),\n",
       " (u'should', 3382),\n",
       " (u'long', 3381),\n",
       " (u'care', 3356),\n",
       " (u'guy', 3339),\n",
       " (u'small', 3337),\n",
       " (u'reason', 3294),\n",
       " (u'--', 3275),\n",
       " (u\"'d\", 3248),\n",
       " (u'side', 3238),\n",
       " (u'old', 3230),\n",
       " (u'found', 3227),\n",
       " (u'select', 3221),\n",
       " (u'meal', 3199),\n",
       " (u'favorit', 3165),\n",
       " (u'week', 3154),\n",
       " (u'start', 3137),\n",
       " (u'differ', 3107),\n",
       " (u'special', 3091),\n",
       " (u'fresh', 3085),\n",
       " (u'both', 3085),\n",
       " (u'awesom', 3068),\n",
       " (u'actual', 3058),\n",
       " (u'live', 3046),\n",
       " (u'close', 3027),\n",
       " (u'dish', 3013),\n",
       " (u'enough', 2995),\n",
       " (u'person', 2976),\n",
       " (u'same', 2956),\n",
       " (u'hot', 2955),\n",
       " (u'him', 2935),\n",
       " (u'these', 2933),\n",
       " (u'offer', 2918),\n",
       " (u'noth', 2912),\n",
       " (u'serv', 2885),\n",
       " (u'stop', 2855),\n",
       " (u'wine', 2825),\n",
       " (u'through', 2820),\n",
       " (u'done', 2820),\n",
       " (u'end', 2818),\n",
       " (u'free', 2814),\n",
       " (u'top', 2792),\n",
       " (u'expect', 2786),\n",
       " (u'pizza', 2773),\n",
       " (u'item', 2773),\n",
       " (u'away', 2771),\n",
       " (u'usual', 2759),\n",
       " (u'whi', 2743),\n",
       " (u'seat', 2730),\n",
       " (u'dinner', 2719),\n",
       " (u'let', 2701),\n",
       " (u'flavor', 2682),\n",
       " (u'kind', 2676),\n",
       " (u'airport', 2667),\n",
       " (u'onc', 2654),\n",
       " (u'anyth', 2646),\n",
       " (u'manag', 2644),\n",
       " (u'pay', 2638),\n",
       " (u'put', 2632),\n",
       " (u'tell', 2615),\n",
       " (u'kid', 2612),\n",
       " (u'open', 2608),\n",
       " (u'thought', 2553),\n",
       " (u'stay', 2544),\n",
       " (u'worth', 2535),\n",
       " (u'beer', 2535),\n",
       " (u'wonder', 2527),\n",
       " (u'fun', 2513),\n",
       " (u'perfect', 2501),\n",
       " (u'those', 2458),\n",
       " (u'thank', 2454),\n",
       " (u'pick', 2449),\n",
       " (u'left', 2440),\n",
       " (u'chang', 2426),\n",
       " (u'buy', 2422),\n",
       " (u'decid', 2407),\n",
       " (u'dog', 2406),\n",
       " (u'excel', 2386),\n",
       " (u'part', 2385),\n",
       " (u'howev', 2380),\n",
       " (u'show', 2349),\n",
       " (u'return', 2348),\n",
       " (u'els', 2344),\n",
       " (u'probabl', 2295),\n",
       " (u'front', 2291),\n",
       " (u'full', 2288),\n",
       " (u'run', 2286),\n",
       " (u'everyon', 2279),\n",
       " (u'coupl', 2274),\n",
       " (u'sit', 2273),\n",
       " (u'famili', 2272),\n",
       " (u'mayb', 2271),\n",
       " (u'qualiti', 2271),\n",
       " (u'job', 2268),\n",
       " (u'super', 2268),\n",
       " (u'deal', 2266),\n",
       " (u'bread', 2257),\n",
       " (u'huge', 2250),\n",
       " (u'10', 2223),\n",
       " (u'keep', 2219),\n",
       " (u'line', 2216),\n",
       " (u'server', 2205),\n",
       " (u'spot', 2186),\n",
       " (u'disappoint', 2176),\n",
       " (u'water', 2172),\n",
       " (u'bring', 2169),\n",
       " (u'atmospher', 2164),\n",
       " (u'each', 2164),\n",
       " (u'move', 2145),\n",
       " (u'quit', 2145),\n",
       " (u'offic', 2139),\n",
       " (u'problem', 2137),\n",
       " (u'month', 2137),\n",
       " (u'without', 2137),\n",
       " (u'cool', 2133),\n",
       " (u'drive', 2129),\n",
       " (u'option', 2109),\n",
       " (u'door', 2105),\n",
       " (u'sever', 2090),\n",
       " (u'insid', 2083),\n",
       " (u'own', 2080),\n",
       " (u'leav', 2079),\n",
       " (u'least', 2050),\n",
       " (u'someon', 2050),\n",
       " (u'larg', 2049),\n",
       " (u'far', 2045),\n",
       " (u'cours', 2042),\n",
       " (u'charg', 2017),\n",
       " (u'breakfast', 2016),\n",
       " (u'talk', 1996),\n",
       " (u'hous', 1995),\n",
       " (u'money', 1993),\n",
       " (u'almost', 1992),\n",
       " (u'dure', 1987),\n",
       " (u'tasti', 1981),\n",
       " (u'outsid', 1981),\n",
       " (u'owner', 1974),\n",
       " (u'town', 1974),\n",
       " (u'meat', 1965),\n",
       " (u'final', 1964),\n",
       " (u'stuff', 1963),\n",
       " (u'name', 1958),\n",
       " (u'local', 1950),\n",
       " (u'decent', 1948),\n",
       " (u'gave', 1944),\n",
       " (u'cook', 1928),\n",
       " (u'hard', 1925),\n",
       " (u'hand', 1904),\n",
       " (u'sweet', 1898),\n",
       " (u'point', 1893),\n",
       " (u'plate', 1884),\n",
       " (u'three', 1882),\n",
       " (u'oh', 1867),\n",
       " (u'less', 1856),\n",
       " (u'burger', 1852),\n",
       " (u'later', 1848),\n",
       " (u'such', 1841),\n",
       " (u'half', 1827),\n",
       " (u'may', 1813),\n",
       " (u'arriv', 1806),\n",
       " (u'watch', 1782),\n",
       " (u'today', 1765),\n",
       " (u'impress', 1765),\n",
       " (u'light', 1758),\n",
       " (u'high', 1756),\n",
       " (u'overal', 1753),\n",
       " (u'fish', 1746),\n",
       " (u'set', 1735),\n",
       " (u'husband', 1731),\n",
       " (u'until', 1729),\n",
       " (u'soup', 1728),\n",
       " (u'abl', 1726),\n",
       " (u'play', 1724),\n",
       " (u'phone', 1720),\n",
       " (u'employe', 1719),\n",
       " (u'quick', 1718),\n",
       " (u'glass', 1711),\n",
       " (u'rice', 1704),\n",
       " (u'ok', 1695),\n",
       " (u'whole', 1694),\n",
       " (u'turn', 1689),\n",
       " (u'coffe', 1688),\n",
       " (u'ye', 1686),\n",
       " (u'must', 1681),\n",
       " (u'complet', 1678),\n",
       " (u'hotel', 1677),\n",
       " (u'chip', 1675),\n",
       " (u'game', 1674),\n",
       " (u'especi', 1670),\n",
       " (u'total', 1666),\n",
       " (u'book', 1666),\n",
       " (u'includ', 1661),\n",
       " (u'girl', 1655),\n",
       " (u'ago', 1646),\n",
       " (u'beauti', 1640),\n",
       " (u'cut', 1637),\n",
       " (u'roll', 1630),\n",
       " (u'fast', 1630),\n",
       " (u'20', 1622),\n",
       " (u'dress', 1615),\n",
       " (u'beef', 1614),\n",
       " (u'dr.', 1595),\n",
       " (u'mean', 1593),\n",
       " (u'hair', 1589),\n",
       " (u'second', 1578),\n",
       " (u'might', 1578),\n",
       " (u'size', 1577),\n",
       " (u'mexican', 1576),\n",
       " (u'issu', 1576),\n",
       " (u'extra', 1574),\n",
       " (u'crowd', 1573),\n",
       " (u'felt', 1562),\n",
       " (u'brought', 1556),\n",
       " (u'plu', 1553),\n",
       " (u'choic', 1551),\n",
       " (u'green', 1548),\n",
       " (u'dine', 1548),\n",
       " (u'wrong', 1547),\n",
       " (u'easi', 1537),\n",
       " (u'purchas', 1533),\n",
       " (u'appoint', 1533),\n",
       " (u'highli', 1528),\n",
       " (u'anyon', 1518),\n",
       " (u'absolut', 1517),\n",
       " (u'wo', 1517),\n",
       " (u'steak', 1508),\n",
       " (u'fantast', 1505),\n",
       " (u'fan', 1503),\n",
       " (u'either', 1496),\n",
       " (u'class', 1495),\n",
       " (u'cake', 1493),\n",
       " (u'cheap', 1492),\n",
       " (u'fill', 1491),\n",
       " (u'rememb', 1490),\n",
       " (u'gener', 1489),\n",
       " (u'music', 1483),\n",
       " (u'fix', 1483),\n",
       " (u'myself', 1483),\n",
       " (u'ladi', 1480),\n",
       " (u'recent', 1477),\n",
       " (u'happen', 1473),\n",
       " (u'dessert', 1471),\n",
       " (u'comfort', 1469),\n",
       " (u'treat', 1464),\n",
       " (u'past', 1458),\n",
       " (u'bean', 1450),\n",
       " (u'cost', 1448),\n",
       " (u'wish', 1448),\n",
       " (u'real', 1447),\n",
       " (u'fact', 1441),\n",
       " (u'nail', 1436),\n",
       " (u'group', 1430),\n",
       " (u'miss', 1430),\n",
       " (u'read', 1426),\n",
       " (u'ice', 1421),\n",
       " (u'extrem', 1417),\n",
       " (u'question', 1417),\n",
       " (u'yet', 1416),\n",
       " (u'guess', 1408),\n",
       " (u'saw', 1405),\n",
       " (u'attent', 1404),\n",
       " (u'seen', 1401),\n",
       " (u'head', 1401),\n",
       " (u'instead', 1391),\n",
       " (u'hope', 1389),\n",
       " (u'although', 1384),\n",
       " (u'card', 1374),\n",
       " (u'regular', 1373),\n",
       " (u'potato', 1368),\n",
       " (u'red', 1356),\n",
       " (u'sign', 1354),\n",
       " (u'piec', 1351),\n",
       " (u'taco', 1350),\n",
       " (u'street', 1349),\n",
       " (u'salsa', 1348),\n",
       " (u'morn', 1345),\n",
       " (u'spici', 1343),\n",
       " (u'pleas', 1341),\n",
       " (u'compani', 1335),\n",
       " (u'yelp', 1332),\n",
       " (u'trip', 1330),\n",
       " (u'surpris', 1325),\n",
       " (u'fine', 1325),\n",
       " (u'date', 1323),\n",
       " (u'often', 1320),\n",
       " (u'waitress', 1297),\n",
       " (u'decor', 1287),\n",
       " (u'sometim', 1285),\n",
       " (u'sale', 1283),\n",
       " (u'wife', 1282),\n",
       " (u'style', 1277),\n",
       " (u'valley', 1271),\n",
       " (u'plenti', 1269),\n",
       " (u'spend', 1268),\n",
       " (u'receiv', 1263),\n",
       " (u'counter', 1262),\n",
       " (u'pool', 1250),\n",
       " (u'parti', 1250),\n",
       " (u'notic', 1250),\n",
       " (u'shrimp', 1248),\n",
       " (u'portion', 1248),\n",
       " (u'late', 1246),\n",
       " (u'grill', 1236),\n",
       " (u'repair', 1232),\n",
       " (u'type', 1226),\n",
       " (u'cream', 1222),\n",
       " (u'weekend', 1222),\n",
       " (u'interest', 1218),\n",
       " (u'etc', 1214),\n",
       " (u'list', 1210),\n",
       " (u'mind', 1207),\n",
       " (u'mention', 1205),\n",
       " (u'appet', 1186),\n",
       " (u'center', 1176),\n",
       " (u'profession', 1176),\n",
       " (u'product', 1176),\n",
       " (u'friday', 1172),\n",
       " (u'believ', 1170),\n",
       " (u'tire', 1166),\n",
       " (u'saturday', 1165),\n",
       " (u'earli', 1165),\n",
       " (u'dri', 1164),\n",
       " (u'eye', 1162),\n",
       " (u'suggest', 1157),\n",
       " (u'bill', 1156),\n",
       " (u'egg', 1153),\n",
       " (u'short', 1149),\n",
       " (u'italian', 1148),\n",
       " (u'man', 1137),\n",
       " (u'bowl', 1137),\n",
       " (u'knowledg', 1136),\n",
       " (u'tip', 1136),\n",
       " (u'okay', 1133),\n",
       " (u'entir', 1132),\n",
       " (u'doctor', 1131),\n",
       " (u'alreadi', 1129),\n",
       " (u'rate', 1127),\n",
       " (u'save', 1126),\n",
       " (u'between', 1125),\n",
       " (u'expens', 1123),\n",
       " (u'explain', 1122),\n",
       " (u'life', 1119),\n",
       " (u'except', 1116),\n",
       " (u'anyway', 1114),\n",
       " (u'rude', 1113),\n",
       " (u'quickli', 1110),\n",
       " (u'desert', 1099),\n",
       " (u'plan', 1096),\n",
       " (u'white', 1095),\n",
       " (u'readi', 1089),\n",
       " (u'provid', 1086),\n",
       " (u'tomato', 1081),\n",
       " (u'wash', 1080),\n",
       " (u'knew', 1080),\n",
       " (u'school', 1079),\n",
       " (u'salon', 1077),\n",
       " (u'stand', 1076),\n",
       " (u'finish', 1074),\n",
       " (u'patio', 1068),\n",
       " (u'fit', 1066),\n",
       " (u'basic', 1064),\n",
       " (u'replac', 1061),\n",
       " (u'chines', 1060),\n",
       " (u'under', 1060),\n",
       " (u'30', 1058),\n",
       " (u'pork', 1056),\n",
       " (u'soon', 1056),\n",
       " (u'hit', 1055),\n",
       " (u'downtown', 1054),\n",
       " (u'bottl', 1053),\n",
       " (u'bought', 1049),\n",
       " (u'floor', 1048),\n",
       " (u'termin', 1043),\n",
       " (u'build', 1042),\n",
       " (u'color', 1041),\n",
       " (u'amount', 1041),\n",
       " (u'share', 1038),\n",
       " (u'wall', 1037),\n",
       " (u'sushi', 1036),\n",
       " (u'wing', 1035),\n",
       " (u'15', 1035),\n",
       " (u'arizona', 1033),\n",
       " (u'rather', 1032),\n",
       " (u'sunday', 1030),\n",
       " (u'gone', 1027),\n",
       " (u'pasta', 1023),\n",
       " (u'pass', 1021),\n",
       " (u'avail', 1021),\n",
       " (u'four', 1021),\n",
       " (u'sat', 1019),\n",
       " (u'event', 1018),\n",
       " (u'sell', 1017),\n",
       " (u'pull', 1016),\n",
       " (u'cold', 1015),\n",
       " (u'serious', 1013),\n",
       " (u'waiter', 1011),\n",
       " (u'near', 1003),\n",
       " (u'understand', 1003),\n",
       " (u'consid', 1002),\n",
       " (u'warm', 1001),\n",
       " (u'inform', 999),\n",
       " (u'given', 997),\n",
       " (u'neighborhood', 997),\n",
       " (u'heard', 986),\n",
       " (u'pack', 984),\n",
       " (u'averag', 984),\n",
       " (u'bartend', 981),\n",
       " (u'paid', 980),\n",
       " (u'behind', 980),\n",
       " (u'oil', 973),\n",
       " (u'kept', 972),\n",
       " (u'onion', 970),\n",
       " (u'drop', 967),\n",
       " (u'mix', 967),\n",
       " (u'smell', 967),\n",
       " (u'bike', 966),\n",
       " (u'consist', 966),\n",
       " (u'cover', 961),\n",
       " (u'chocol', 958),\n",
       " (u'bathroom', 950),\n",
       " (u'bite', 947),\n",
       " (u'prepar', 943),\n",
       " (u'along', 943),\n",
       " (u'across', 940),\n",
       " (u'twice', 938),\n",
       " (u'taken', 938),\n",
       " (u'number', 931),\n",
       " (u'within', 931),\n",
       " (u'garden', 930),\n",
       " (u'incred', 927),\n",
       " (u'answer', 926),\n",
       " (u'appreci', 923),\n",
       " (u'lack', 922),\n",
       " (u'world', 919),\n",
       " (u'entre', 918),\n",
       " (u'varieti', 911),\n",
       " (u'hear', 911),\n",
       " (u'meet', 908),\n",
       " (u'veggi', 904),\n",
       " (u'bag', 904),\n",
       " (u'thai', 903),\n",
       " (u'yummi', 902),\n",
       " (u'bbq', 897),\n",
       " (u'idea', 889),\n",
       " (u'tv', 888),\n",
       " (u'normal', 886),\n",
       " (u'season', 885),\n",
       " (u'sound', 880),\n",
       " (u'add', 879),\n",
       " (u'slice', 879),\n",
       " (u'kitchen', 877),\n",
       " (u'greet', 875),\n",
       " (u'space', 874),\n",
       " (u'possibl', 874),\n",
       " (u'tea', 872),\n",
       " (u'due', 871),\n",
       " (u'state', 870),\n",
       " (u'anim', 868),\n",
       " (u'terribl', 868),\n",
       " (u'five', 867),\n",
       " (u'ate', 866),\n",
       " (u'typic', 866),\n",
       " (u'citi', 862),\n",
       " (u'secur', 860),\n",
       " (u'rest', 859),\n",
       " (u'exactli', 859),\n",
       " (u'complaint', 856),\n",
       " (u'dollar', 855),\n",
       " (u'worst', 854),\n",
       " (u'face', 852),\n",
       " (u'itself', 851),\n",
       " (u'welcom', 847),\n",
       " (u'slow', 846),\n",
       " (u'case', 844),\n",
       " (u'box', 843),\n",
       " (u'simpl', 842),\n",
       " (u'bruschetta', 841),\n",
       " (u'write', 841),\n",
       " (u'note', 840),\n",
       " (u'figur', 839),\n",
       " (u'french', 838),\n",
       " (u'reserv', 834),\n",
       " (u'word', 833),\n",
       " (u'grab', 832),\n",
       " (u'rock', 830),\n",
       " (u'authent', 827),\n",
       " (u'perfectli', 826),\n",
       " (u'base', 824),\n",
       " (u'follow', 824),\n",
       " (u'burrito', 822),\n",
       " (u'club', 822),\n",
       " (u'view', 821),\n",
       " (u'horribl', 820),\n",
       " (u'immedi', 817),\n",
       " (u'desk', 816),\n",
       " (u'addit', 813),\n",
       " (u'realiz', 810),\n",
       " (u'pet', 809),\n",
       " (u'chain', 806),\n",
       " (u'relax', 806),\n",
       " (u'ton', 806),\n",
       " (u'anywher', 805),\n",
       " (u'shoe', 804),\n",
       " (u'choos', 804),\n",
       " (u'chair', 803),\n",
       " (u'spent', 800),\n",
       " (u'glad', 797),\n",
       " (u'hate', 794),\n",
       " (u'learn', 792),\n",
       " (u'mile', 791),\n",
       " (u'onlin', 788),\n",
       " (u'unfortun', 787),\n",
       " (u'cloth', 786),\n",
       " (u'patient', 786),\n",
       " (u'wow', 782),\n",
       " (u'low', 781),\n",
       " (u'az', 777),\n",
       " (u'exhibit', 777),\n",
       " (u'la', 777),\n",
       " (u'rib', 777),\n",
       " (u'conveni', 774),\n",
       " (u'ad', 772),\n",
       " (u'buck', 769),\n",
       " (u'cute', 767),\n",
       " (u'mom', 767),\n",
       " (u'compar', 766),\n",
       " (u'pleasant', 765),\n",
       " (u'main', 765),\n",
       " (u'continu', 764),\n",
       " (u'noodl', 764),\n",
       " (u'bed', 760),\n",
       " (u'bacon', 758),\n",
       " (u'hold', 756),\n",
       " (u'ride', 756),\n",
       " (u'zoo', 754),\n",
       " (u'cafe', 754),\n",
       " (u'daughter', 754),\n",
       " (u'sort', 753),\n",
       " (u'cup', 752),\n",
       " (u'standard', 751),\n",
       " (u'central', 749),\n",
       " (u'black', 749),\n",
       " (u'section', 748),\n",
       " (u'window', 748),\n",
       " (u'market', 748),\n",
       " (u'afternoon', 748),\n",
       " (u'travel', 747),\n",
       " (u'coupon', 747),\n",
       " (u'fee', 747),\n",
       " (u'gift', 745),\n",
       " (u'truli', 745),\n",
       " (u'sport', 741),\n",
       " (u'pepper', 741),\n",
       " (u'birthday', 740),\n",
       " (u'longer', 737),\n",
       " (u'chili', 736),\n",
       " (u'matter', 734),\n",
       " (u'excit', 731),\n",
       " (u'yourself', 731),\n",
       " (u'wed', 725),\n",
       " (u'fair', 722),\n",
       " (u'madison', 722),\n",
       " (u'babi', 721),\n",
       " (u'son', 720),\n",
       " (u'goe', 719),\n",
       " (u'movi', 719),\n",
       " (u'prefer', 717),\n",
       " (u'vet', 716),\n",
       " (u'woman', 715),\n",
       " (u'flight', 715),\n",
       " (u'mall', 713),\n",
       " (u'dirti', 712),\n",
       " (u'updat', 712),\n",
       " (u'hang', 711),\n",
       " (u'scottsdal', 704),\n",
       " (u'honest', 704),\n",
       " (u'cash', 703),\n",
       " (u'dark', 699),\n",
       " (u'ambianc', 698),\n",
       " (u'credit', 698),\n",
       " (u'unless', 697),\n",
       " (u'rush', 695),\n",
       " (u'schedul', 695),\n",
       " (u'smoke', 694),\n",
       " (u'discount', 694),\n",
       " (u'abov', 694),\n",
       " (u'sorri', 693),\n",
       " (u'upon', 693),\n",
       " (u'sun', 692),\n",
       " (u'poor', 691),\n",
       " (u'mine', 691),\n",
       " (u'machin', 689),\n",
       " (u'uniqu', 688),\n",
       " (u'level', 681),\n",
       " (u'agre', 681),\n",
       " (u'speak', 681),\n",
       " (u'buffet', 676),\n",
       " (u'whatev', 676),\n",
       " (u'complain', 674),\n",
       " (u'summer', 673),\n",
       " (u'rare', 672),\n",
       " (u'suppos', 670),\n",
       " (u'limit', 670),\n",
       " (u'suck', 669),\n",
       " (u'diner', 668),\n",
       " (u'smile', 668),\n",
       " (u'member', 667),\n",
       " (u'true', 665),\n",
       " (u'break', 665),\n",
       " (u'train', 664),\n",
       " (u'massag', 664),\n",
       " (u'valu', 662),\n",
       " (u'wrap', 662),\n",
       " (u'allow', 661),\n",
       " (u'togeth', 661),\n",
       " (u'chanc', 661),\n",
       " (u'appl', 658),\n",
       " (u'suit', 655),\n",
       " (u'heat', 654),\n",
       " (u'shot', 651),\n",
       " (u'corn', 650),\n",
       " (u'empti', 646),\n",
       " (u'simpli', 645),\n",
       " (u'request', 645),\n",
       " (u'frequent', 644),\n",
       " (u'blue', 644),\n",
       " (u'cooki', 643),\n",
       " (u'touch', 643),\n",
       " (u'die', 643),\n",
       " (u'corner', 642),\n",
       " (u'eaten', 641),\n",
       " (u'ticket', 640),\n",
       " (u'origin', 639),\n",
       " (u'air', 639),\n",
       " (u'deliv', 639),\n",
       " (u'appar', 637),\n",
       " (u'ring', 636),\n",
       " (u'apart', 636),\n",
       " (u'road', 636),\n",
       " (u'folk', 635),\n",
       " (u'carri', 634),\n",
       " (u'brand', 634),\n",
       " (u'pair', 633),\n",
       " (u'50', 633),\n",
       " (u'satisfi', 632),\n",
       " (u'stock', 629),\n",
       " (u'yeah', 626),\n",
       " (u'liter', 626),\n",
       " (u'certainli', 625),\n",
       " (u'boyfriend', 623),\n",
       " (u'rental', 622),\n",
       " (u'pricey', 622),\n",
       " (u'pictur', 621),\n",
       " (u'sausag', 619),\n",
       " (u'soda', 618),\n",
       " (u'detail', 617),\n",
       " (u'wear', 617),\n",
       " (u'trust', 616),\n",
       " (u'groceri', 613),\n",
       " (u'beat', 613),\n",
       " (u'joint', 612),\n",
       " (u'gym', 610),\n",
       " (u'avoid', 609),\n",
       " (u'dip', 609),\n",
       " (u'dive', 607),\n",
       " (u'process', 606),\n",
       " (u'art', 605),\n",
       " (u'stori', 600),\n",
       " (u'gate', 600),\n",
       " (u'lost', 598),\n",
       " (u'young', 597),\n",
       " (u'gotten', 594),\n",
       " (u'american', 593),\n",
       " (u'convers', 593),\n",
       " (u'board', 593),\n",
       " (u'middl', 593),\n",
       " (u'loud', 590),\n",
       " (u'forward', 590),\n",
       " (u'present', 589),\n",
       " (u'practic', 588),\n",
       " (u'tortilla', 587),\n",
       " (u'bake', 587),\n",
       " (u'bland', 586),\n",
       " (u'boy', 585),\n",
       " (u'remind', 585),\n",
       " (u'establish', 584),\n",
       " (u'guest', 584),\n",
       " (u'fairli', 582),\n",
       " (u'annoy', 580),\n",
       " (u'futur', 579),\n",
       " (u'kinda', 578),\n",
       " (u'min', 577),\n",
       " (u'unit', 576),\n",
       " (u'system', 575),\n",
       " (u'vehicl', 574),\n",
       " (u'100', 574),\n",
       " (u'plant', 574),\n",
       " (u'organ', 573),\n",
       " (u'met', 571),\n",
       " (u'wast', 571),\n",
       " (u'crave', 571),\n",
       " (u'listen', 568),\n",
       " (u'band', 568),\n",
       " (u'imagin', 568),\n",
       " (u'danc', 566),\n",
       " (u'paint', 565),\n",
       " (u'pedicur', 564),\n",
       " (u'posit', 562),\n",
       " (u'postino', 562),\n",
       " (u'mostli', 561),\n",
       " (u'chicago', 561),\n",
       " (u'ingredi', 559),\n",
       " (u'pain', 558),\n",
       " (u'step', 558),\n",
       " (u'mouth', 557),\n",
       " (u'becom', 557),\n",
       " (u'crazi', 557),\n",
       " (u'seafood', 557),\n",
       " (u'phx', 556),\n",
       " (u'major', 556),\n",
       " (u'afford', 554),\n",
       " (u'beyond', 554),\n",
       " (u'forget', 553),\n",
       " (u'insur', 551),\n",
       " (u'appear', 550),\n",
       " (u'stick', 550),\n",
       " (u'concern', 549),\n",
       " (u'enter', 549),\n",
       " (u'easili', 548),\n",
       " (u'fli', 546),\n",
       " (u'somewher', 545),\n",
       " (u'design', 545),\n",
       " (u'post', 545),\n",
       " (u'honestli', 545),\n",
       " (u'singl', 545),\n",
       " (u'attend', 544),\n",
       " (u'opinion', 543),\n",
       " (u'tini', 543),\n",
       " (u'garlic', 542),\n",
       " (u'rang', 542),\n",
       " (u'monday', 540),\n",
       " (u'import', 539),\n",
       " (u'older', 539),\n",
       " (u'crispi', 539),\n",
       " (u'vegetarian', 537),\n",
       " (u'attitud', 537),\n",
       " (u'butter', 534),\n",
       " (u'children', 534),\n",
       " (u'accommod', 533),\n",
       " (u'fabul', 533),\n",
       " (u'heart', 533),\n",
       " (u'situat', 532),\n",
       " (u'deli', 531),\n",
       " (u'ridicul', 531),\n",
       " (u'age', 529),\n",
       " (u'25', 528),\n",
       " (u'salmon', 527),\n",
       " (u'quot', 527),\n",
       " (u'enchilada', 523),\n",
       " (u'bother', 523),\n",
       " (u'margarita', 522),\n",
       " (u'specif', 521),\n",
       " (u'outdoor', 520),\n",
       " (u'facil', 520),\n",
       " (u'nearli', 520),\n",
       " (u'strip', 518),\n",
       " (u'ahead', 517),\n",
       " (u'instal', 516),\n",
       " (u'weird', 515),\n",
       " (u'rent', 515),\n",
       " (u'cat', 514),\n",
       " (u'sent', 513),\n",
       " (u'countri', 513),\n",
       " (u'sky', 513),\n",
       " (u'support', 512),\n",
       " (u'per', 511),\n",
       " (u'hospit', 511),\n",
       " (u'tender', 507),\n",
       " (u'caus', 507),\n",
       " (u'paper', 505),\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted=sorted(vocab.items(), key=operator.itemgetter(1),reverse=True)\n",
    "count=1\n",
    "for tup in vocab_sorted[:20]:\n",
    "    #print count,tup[0],tup[1]\n",
    "    count+=1\n",
    "#len(vocab_sorted) #1716\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "Recall in class our discussion of Zipf's law. Let's see if this law applies to our Yelp reviews. You should use matplotlib to plot the log-base10 term counts on the y-axis versus the log-base10 rank on the x-axis. Your aim is to create a figure like the one in Figure 5.2 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKSL5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdb86390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt. plot([math.log10(r) for r in range(1,len(vocab_sorted)+1)], [math.log10(f[1]) for f in vocab_sorted])\n",
    "plt.xlabel('log(rank)')\n",
    "plt.ylabel('log(frequency)')\n",
    "plt.title(\"Zipf's Law\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Is this consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes, it is consistent with Zipf's law. I understand how terms are distributed across documents. So if the most frequent term occurs f1 times, then the second most frequent term has half as many occurrences, the third most frequent term a third as many occurrences, and so on. The intuition is that frequency decreases very rapidly with rank and the collection frequency fi of the ith most common term is proportional to 1/i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Feature Represenation (10 points)\n",
    "\n",
    "In this part you will build feature vectors for each review. This will be input to our ML classifiers. You should call your parser from earlier, using all the same assumptions (e.g., casefolding, stemming). Each feature value should be the term count for that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def create_vector(train_set,test_set):\n",
    "    cv = CountVectorizer(encoding='utf-8')\n",
    "    train_vec=cv.fit_transform(' '.join(s) for s in train_set)\n",
    "    test_vec=cv.transform(' '.join(s) for s in test_set)\n",
    "    print (len(cv.get_feature_names()))\n",
    "    return (train_vec,test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Machine Learning Basics (30 points)\n",
    "\n",
    "In this part you will evaluate a bunch of classifiers -- kNN, Decision tree, Naive Bayes, and SVM -- on the feature vectors generated in the previous task in two different settings. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.**\n",
    "\n",
    "### Setting 1: Splitting data into train-test \n",
    "\n",
    "In the first setting, you should treat the first 70% of your data as training. The remaining 30% should be for testing. \n",
    "\n",
    "### Setting 2: Using 5 fold cross-validation\n",
    "\n",
    "In the second setting, use 5-folk cross-validation. \n",
    "\n",
    "### What to report\n",
    "\n",
    "* Report the overall accuracy for both settings.\n",
    "* For the class \"Food-relevant\", report the precision and recall for both settings.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall for both settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32191\n"
     ]
    }
   ],
   "source": [
    "#SETTING - 1\n",
    "train_data=reviews[:int(0.7*len(reviews))]\n",
    "test_data=reviews[int(0.7*len(reviews)):]\n",
    "X_train_vec,X_test_vec=create_vector(train_data,test_data)\n",
    "Y_train=labels[:int(0.7*len(reviews))]\n",
    "Y_test=labels[int(0.7*len(reviews)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.32275\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.31      0.47     11761\n",
      "  Food-relevant       0.03      0.97      0.05       239\n",
      "\n",
      "    avg / total       0.98      0.32      0.46     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train_vec,Y_train)\n",
    "predictions=knn.predict(X_test_vec)\n",
    "target_names = ['Food-irrelevant', 'Food-relevant']\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.861833333333\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.86      0.92     11761\n",
      "  Food-relevant       0.12      0.98      0.22       239\n",
      "\n",
      "    avg / total       0.98      0.86      0.91     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn import svm\n",
    "linear_svm = svm.LinearSVC()\n",
    "linear_svm.fit(X_train_vec, Y_train)\n",
    "predictions = linear_svm.predict(X_test_vec)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.861833333333\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE\n",
    "from sklearn import tree\n",
    "dec_tree = tree.DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.93775\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.94      0.97     11761\n",
      "  Food-relevant       0.24      0.98      0.39       239\n",
      "\n",
      "    avg / total       0.98      0.94      0.96     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NAIVE BAYES\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec,Y_train)\n",
    "predictions=mnb.predict(X_test_vec)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.880916666667\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.88      0.94     11761\n",
      "  Food-relevant       0.14      0.99      0.25       239\n",
      "\n",
      "    avg / total       0.98      0.88      0.92     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_vec, Y_train)\n",
    "predictions = logreg.predict(X_test_vec)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SETTING - 2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X_new=reviews\n",
    "Y_new=labels\n",
    "cv2 = CountVectorizer(encoding='utf-8')\n",
    "X_new_vec=cv2.fit_transform(' '.join(s) for s in X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "scores=cross_val_score(knn,X_new_vec,Y_new,cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "scores=cross_val_score(linear_svm,X_new_vec,Y_new,cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#Decsion Tree\n",
    "scores=cross_val_score(dec_tree,X_new_vec,Y_new,cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "#NAIVE BAYES\n",
    "scores=cross_val_score(mnb,X_new_vec,Y_new,cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "scores=cross_val_score(logreg,X_new_vec,Y_new,cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Analyzing your results (5 points) \n",
    "\n",
    "OK, now that you have tried four different classifiers, what do you observe? Any conclusions you can draw? Give us one or two paragraphs summarizing your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried with 5 models. NB has proved to be robust to noise and missing data as it has the ability of performing the probabilities without having any impact on the final outcome.\n",
    "Despite its powers discussed above, SVM tends to be computationally expensive by virtue of the kernel technique it employs during learning. This however can be minimized during SVM model training and evaluation since the kernel is a parameter that can be adjusted depending on the performance, which eventually reduces computational cost.  outstanding performance of SVM could be attributed to a number factors: majority of TC problems are mostly linearly separable, and SVM employs threshold functions to develop margins that linearly separate the Classes; SVMs tend to use over-fitting protection mechanism that is independent of the dimensionality of the feature space, thus, the number of features tends not to be an issue; and SVMs are well designed to deal with sparseness found in feature vectors.\n",
    "DT is characterised by its relative transparent outputs, which are easy to be read and understood by humans. DT has been shown to have superior performance over other techniques for this dataset.\n",
    "\n",
    "Method - Decision Tree\n",
    "Advantage - Easily pick the best feature from the set of data\n",
    "Disadvantage - Overfilling is the problem and not so much accurate.\n",
    "\n",
    "Method - K Nearest Neighbor \n",
    "Advantage - even robust to noisy training data.\n",
    "Disadvantage - Classification time is too long and it is difficult to find the optimal value of k.\n",
    "\n",
    "Method - Naive Bayes \n",
    "Advantage - Easy to implement and it requires less amount of training data.\n",
    "Disadvantage - The assumption of independence of the class results in loss of accuracy.\n",
    "\n",
    "Method - SVM\n",
    "Advantage - Robust and very accurate.\n",
    "Disadvantage - High algorithmic complexity and extensive memory requirements for large scale task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Improving your classifier (10 points)\n",
    "\n",
    "I think we can do better! In this part, your job is to create new features that you can think can help improve your classifier. You may choose to use new weightings for your words, new derived features (e.g., count of 3-letter words), or whatever you like. You may also add in the extra features in the json: funny, useful, cool. You will need to experiment with different approaches ... once you finalize on your best approach, include the features here with a description (that is, tell us what the feature means). Then give us your classifier results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instead of count vectorizer using TF-IDF vectorizer and using bi grams, removing stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def create_vector(train_set,test_set):\n",
    "    tf = TfidfVectorizer(encoding='utf-8',ngram_range=(1, 2),stop_words='english')\n",
    "    train_vec=tf.fit_transform(' '.join(s) for s in train_set)\n",
    "    test_vec=tf.transform(' '.join(s) for s in test_set)\n",
    "    print (len(tf.get_feature_names()))\n",
    "    return (train_vec,test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803313\n"
     ]
    }
   ],
   "source": [
    "#SETTING - 1\n",
    "train_data=reviews[:int(0.7*len(reviews))]\n",
    "test_data=reviews[int(0.7*len(reviews)):]\n",
    "X_train_vec,X_test_vec=create_vector(train_data,test_data)\n",
    "Y_train=labels[:int(0.7*len(reviews))]\n",
    "Y_test=labels[int(0.7*len(reviews)):]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.71025\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.70      0.83     11761\n",
      "  Food-relevant       0.06      0.97      0.12       239\n",
      "\n",
      "    avg / total       0.98      0.71      0.81     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train_vec,Y_train)\n",
    "predictions=knn.predict(X_test_vec)\n",
    "target_names = ['Food-irrelevant', 'Food-relevant']\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.917916666667\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.92      0.96     11761\n",
      "  Food-relevant       0.19      0.99      0.32       239\n",
      "\n",
      "    avg / total       0.98      0.92      0.94     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "linear_svm = svm.LinearSVC()\n",
    "linear_svm.fit(X_train_vec, Y_train)\n",
    "predictions = linear_svm.predict(X_test_vec)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.917916666667\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE\n",
    "dec_tree = tree.DecisionTreeClassifier(criterion='gini',max_depth=7)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.126\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.11      0.20     11761\n",
      "  Food-relevant       0.02      1.00      0.04       239\n",
      "\n",
      "    avg / total       0.98      0.13      0.19     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NAIVE BAYES\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec,Y_train)\n",
    "predictions=mnb.predict(X_test_vec)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test set is  0.882416666667\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       1.00      0.88      0.94     11761\n",
      "  Food-relevant       0.14      1.00      0.25       239\n",
      "\n",
      "    avg / total       0.98      0.88      0.92     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_vec, Y_train)\n",
    "predictions = logreg.predict(X_test_vec)\n",
    "print 'Model Accuracy on Test set is ', metrics.accuracy_score(Y_test, predictions)\n",
    "print classification_report(Y_test, predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTING - 2\n",
    "from sklearn.metrics import *\n",
    "def cv_parameters(y_actual,prediction):\n",
    "    print classification_report(y_actual,prediction,target_names=target_names)\n",
    "    return accuracy_score(y_actual,prediction) \n",
    "X_new=reviews\n",
    "Y_new=labels\n",
    "tf2 = TfidfVectorizer(encoding='utf-8',ngram_range=(1, 2))\n",
    "X_new_vec=tf2.fit_transform(' '.join(s) for s in X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.85      0.84      0.85      4000\n",
      "  Food-relevant       0.84      0.85      0.85      4000\n",
      "\n",
      "    avg / total       0.85      0.85      0.85      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.91      0.82      0.87      4000\n",
      "  Food-relevant       0.84      0.92      0.88      4000\n",
      "\n",
      "    avg / total       0.88      0.87      0.87      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.88      0.92      0.90      4000\n",
      "  Food-relevant       0.91      0.87      0.89      4000\n",
      "\n",
      "    avg / total       0.90      0.90      0.90      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.86      0.94      0.90      4000\n",
      "  Food-relevant       0.93      0.84      0.89      4000\n",
      "\n",
      "    avg / total       0.90      0.89      0.89      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.87      0.94      0.91      4000\n",
      "  Food-relevant       0.94      0.86      0.90      4000\n",
      "\n",
      "    avg / total       0.91      0.90      0.90      8000\n",
      "\n",
      "Accuracy: 0.88 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "scores=cross_val_score(knn,X_new_vec,Y_new,cv=5,scoring=make_scorer(cv_parameters))\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.91      0.94      0.92      4000\n",
      "  Food-relevant       0.93      0.91      0.92      4000\n",
      "\n",
      "    avg / total       0.92      0.92      0.92      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.94      0.93      0.93      4000\n",
      "  Food-relevant       0.93      0.94      0.93      4000\n",
      "\n",
      "    avg / total       0.93      0.93      0.93      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.94      0.97      0.95      4000\n",
      "  Food-relevant       0.97      0.94      0.95      4000\n",
      "\n",
      "    avg / total       0.95      0.95      0.95      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.94      0.97      0.96      4000\n",
      "  Food-relevant       0.97      0.94      0.95      4000\n",
      "\n",
      "    avg / total       0.96      0.95      0.95      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.95      0.98      0.96      4000\n",
      "  Food-relevant       0.98      0.94      0.96      4000\n",
      "\n",
      "    avg / total       0.96      0.96      0.96      8000\n",
      "\n",
      "Accuracy: 0.94 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "scores=cross_val_score(linear_svm,X_new_vec,Y_new,cv=5,scoring=make_scorer(cv_parameters))\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.76      0.90      0.82      4000\n",
      "  Food-relevant       0.88      0.71      0.79      4000\n",
      "\n",
      "    avg / total       0.82      0.81      0.80      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.79      0.94      0.86      4000\n",
      "  Food-relevant       0.92      0.75      0.83      4000\n",
      "\n",
      "    avg / total       0.86      0.84      0.84      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.78      0.96      0.86      4000\n",
      "  Food-relevant       0.94      0.73      0.82      4000\n",
      "\n",
      "    avg / total       0.86      0.84      0.84      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.77      0.88      0.82      4000\n",
      "  Food-relevant       0.86      0.73      0.79      4000\n",
      "\n",
      "    avg / total       0.81      0.81      0.80      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.80      0.94      0.86      4000\n",
      "  Food-relevant       0.93      0.76      0.83      4000\n",
      "\n",
      "    avg / total       0.86      0.85      0.85      8000\n",
      "\n",
      "Accuracy: 0.83 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "#Decsion Tree\n",
    "scores=cross_val_score(dec_tree,X_new_vec,Y_new,cv=5,scoring=make_scorer(cv_parameters))\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.92      0.87      0.90      4000\n",
      "  Food-relevant       0.88      0.93      0.90      4000\n",
      "\n",
      "    avg / total       0.90      0.90      0.90      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.94      0.88      0.91      4000\n",
      "  Food-relevant       0.89      0.95      0.92      4000\n",
      "\n",
      "    avg / total       0.92      0.91      0.91      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.95      0.95      0.95      4000\n",
      "  Food-relevant       0.95      0.94      0.95      4000\n",
      "\n",
      "    avg / total       0.95      0.95      0.95      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.95      0.95      0.95      4000\n",
      "  Food-relevant       0.95      0.95      0.95      4000\n",
      "\n",
      "    avg / total       0.95      0.95      0.95      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.95      0.96      0.96      4000\n",
      "  Food-relevant       0.96      0.95      0.96      4000\n",
      "\n",
      "    avg / total       0.96      0.96      0.96      8000\n",
      "\n",
      "Accuracy: 0.93 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "#NAIVE BAYES\n",
    "scores=cross_val_score(mnb,X_new_vec,Y_new,cv=5,scoring=make_scorer(cv_parameters))\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.90      0.93      0.91      4000\n",
      "  Food-relevant       0.93      0.89      0.91      4000\n",
      "\n",
      "    avg / total       0.91      0.91      0.91      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.92      0.93      0.93      4000\n",
      "  Food-relevant       0.93      0.92      0.93      4000\n",
      "\n",
      "    avg / total       0.93      0.93      0.93      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.93      0.97      0.95      4000\n",
      "  Food-relevant       0.97      0.92      0.95      4000\n",
      "\n",
      "    avg / total       0.95      0.95      0.95      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.93      0.96      0.95      4000\n",
      "  Food-relevant       0.96      0.93      0.94      4000\n",
      "\n",
      "    avg / total       0.95      0.95      0.95      8000\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Food-irrelevant       0.93      0.97      0.95      4000\n",
      "  Food-relevant       0.97      0.93      0.95      4000\n",
      "\n",
      "    avg / total       0.95      0.95      0.95      8000\n",
      "\n",
      "Accuracy: 0.94 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "scores=cross_val_score(logreg,X_new_vec,Y_new,cv=5,scoring=make_scorer(cv_parameters))\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have experimented with TF-IDF instead of CountVectorizer, used bigrams, removed stop-words. After these changes, i observed that accuracy was increased by 2 percent and also computation time have increased because of considering bigrams because of which feature dimension was increased.\n",
    "I have tried with 5 models. Top performer, Naive Bayes has proved to be robust to noise and missing data as it has the ability of performing the probabilities without having any impact on the final outcome. Despite its powers discussed above, SVM tends to be computationally expensive by virtue of the kernel technique it employs during learning. This however can be minimized during SVM model training and evaluation since the kernel is a parameter that can be adjusted depending on the performance, which eventually reduces computational cost. outstanding performance of SVM could be attributed to a number factors: majority of TC problems are mostly linearly separable, and SVM employs threshold functions to develop margins that linearly separate the Classes; SVMs tend to use over-fitting protection mechanism that is independent of the dimensionality of the feature space, thus, the number of features tends not to be an issue; and SVMs are well designed to deal with sparseness found in feature vectors. DT is characterised by its relative transparent outputs, which are easy to be read and understood by humans. DT has been shown to have superior performance over other techniques for this dataset.\n",
    "\n",
    "From these observations, for the given data for this assignment, accuracy is varied as below:\n",
    "NB > SVM > Decision Tree > K-NN\n",
    "\n",
    "Method - Decision Tree:\n",
    "Advantage - Easily pick the best feature from the set of data \n",
    "Disadvantage - Overfilling is the problem and not so much accurate.\n",
    "\n",
    "Method - K Nearest Neighbor:\n",
    "Advantage - even robust to noisy training data. \n",
    "Disadvantage - Classification time is too long and it is difficult to find the optimal value of k.\n",
    "\n",
    "Method - Naive Bayes:\n",
    "Advantage - Easy to implement and it requires less amount of training data, gave good results compared to others. \n",
    "Disadvantage - The assumption of independence of the class results in loss of accuracy.\n",
    "\n",
    "Method - SVM:\n",
    "Advantage - Robust and very accurate. \n",
    "Disadvantage - High algorithmic complexity and extensive memory requirements for large scale task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: What are the most informative features in distinguishing these two classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. I have experimented with TF-IDF instead of CountVectorizer, used bigrams, removed stop-words. After these changes, i observed that accuracy was increased by 2 percent and also computation time have increased because of considering bigrams because of which feature dimension was increased.\n",
    "2. I have tried adding features like votes for funny, useful, cool. \n",
    "3. I have used a threshold to weigh words which are repeating and assign values according to the threshold. I observed that weighing words which are 1 letter and 10 letter words with lesser weight have improved the results. \n",
    "4. Although, I have considered votes of funny, useful, cool, stars but it did not help much in increasing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Learning to Rank (30 points)\n",
    "\n",
    "For this part, we're going to play with some Microsoft LETOR data that has query-document relevance judgments. Let's see how learning to rank works in practice. \n",
    "\n",
    "First, you will need to download the MQ2008.zip file from the Resources tab on Piazza. This is data from the [Microsoft Research IR Group](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/).\n",
    "\n",
    "The data includes 15,211 rows. Each row is a query-document pair. The first column is a relevance label of this pair (0,1 or 2--> the higher value the more related), the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. A query-document pair is represented by a 46-dimensional feature vector. Features are a numeric value describing a document and query such as TFIDF, BM25, Page Rank, .... You can find compelete description of features from [here](https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf).\n",
    "\n",
    "The good news for you is the dataset is ready for analysis: It has already been split into 5 folds (see the five folders called Fold1, ..., Fold5).\n",
    "\n",
    "For this assignment, we're going to leave our favorite scikit-learn and instead use [SVM-rank](https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html). This is the basic ranking SVM we talked about in class. You'll see that SVM-rank considers pairwise relevance between docs -- so based on the training data it will transform the data into pairs -- like D1 > D2 and then learn a separator.\n",
    "\n",
    "\n",
    "## Part 2.1: Optimizing SVM-Rank (15 points)\n",
    "\n",
    "First, you should explore how the different parameters affect the quality of the Ranking SVM. You'll see that you can vary the kernel function, the loss function and so forth. \n",
    "\n",
    "You should run SVM-Rank using the default options over each of the five folds. You should find the error on the test set (for example, depending on your settings, svm_rank_classify will give you the zero/one error statistics (that is, the number of correct pairs and the number of incorrect pairs). Report the average. \n",
    "\n",
    "Then try different parameters and report how they impact the quality of results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average error on Test set for 5 fold cross validation are:\n",
    "t=0 c=2.0 Zero/one-error on test set: 58.97% \n",
    "t=0 c=2.4 Zero/one-error on test set: 58.33% \n",
    "t=0 c=0.2 Zero/one-error on test set: 59.62% \n",
    "t=3 c=1.6 Zero/one-error on test set: 66.67%\n",
    "t=2 c=2.2 g=0.01 Zero/one-error on test set: 67.67%\n",
    "t=1 c=3.2 d=2 Zero/one-error on test set: 61.67%\n",
    "t=1 c=3.8 d=2 Zero/one-error on test set: 60.67%\n",
    "t=0 c=5.2 Zero/one-error on test set: 59.48%\n",
    "t=0 c=10.0 Zero/one-error on test set: 60.26% \n",
    "\n",
    "I observed that SVM accuracy is not that good although i have used different kernels. Specifically, polynomial SVM gave bad results and RBF,Linear gave good results compared to other kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2.1: Noise! (15 points)\n",
    "\n",
    "Now we're going to investigate whether the ranking SVM is easily influenced by noisy features. For example, what if some of the features you have are in error? Or what if you downloaded only a portion of a page to calculate a feature? (so the count of inlinks would be wrong)? \n",
    "\n",
    "In this case, add some noise to the features. What happens to the results? You may choose to add random noise throughout, noise to a single feature, noise to multiple features, etc. The choices are up to you. We aim to see what kind of exploration you conduct and what you conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added random noise using the below script:\n",
    "\n",
    "file_path='MQ2008/MQ2008/Fold1/train.txt'\n",
    "from random import randint\n",
    "import random\n",
    "with open(file_path) as fn:\n",
    "        for line in fn:\n",
    "            row=line.split()\n",
    "            rand_feature=randint(1, 46)\n",
    "            rand_value=random.uniform(0,100)\n",
    "            row[rand_feature+1]=str(rand_feature)+':'+str(rand_value)\n",
    "            new_line=' '.join(row)\n",
    "            with open(\"MQ2008/MQ2008/Fold1/train_edited.txt\", \"a\") as file_new: \n",
    "                    file_new.write(new_line+'\\n')\n",
    "                    \n",
    "This script selects one of the 46 features at random and chooses a value at random and writes in to a new train_edited file.There was 3% drop in accuracy when i added random values in the range of (0,100). For some random noise where values in the range of (0,1), sometimes accuracy increased by 1%. Whrn random values in the range of (0,1000) are added, accuracy decreased by 20 percent.\n",
    "Later tried varying the multiple column values and found that accuracy is varied by 1-3% accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
